#!/usr/bin/env python3
"""
RunPod Handler - å¼ºåˆ¶GPUæ¨¡å¼ï¼Œå½»åº•åˆ é™¤CPUä»£ç 
ä¸“ä¸ºL40 GPUä¼˜åŒ–ï¼Œ45GBæ˜¾å­˜
"""

import runpod
import os
import logging
import time
import subprocess
import json
from typing import Optional, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
model = None
model_type = None
model_path = None

# å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['LLAMA_CUBLAS'] = '1'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

def check_gpu_usage():
    """æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(', ')
            gpu_util = gpu_info[0]
            memory_used = float(gpu_info[1]) / 1024  # GB
            memory_total = float(gpu_info[2]) / 1024  # GB
            temperature = gpu_info[3]
            logger.info(f"ğŸ”¥ GPUçŠ¶æ€: åˆ©ç”¨ç‡{gpu_util}%, æ˜¾å­˜{memory_used:.1f}/{memory_total:.1f}GB, æ¸©åº¦{temperature}Â°C")
            return True
        else:
            logger.error("æ— æ³•è·å–GPUçŠ¶æ€")
            return False
    except Exception as e:
        logger.error(f"GPUçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_gpu():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    try:
        # æ£€æŸ¥PyTorch CUDAæ”¯æŒ
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if gpu_count > 0 else 0
            logger.info(f"âœ… GPUæ£€æµ‹æˆåŠŸ: {gpu_name}, æ˜¾å­˜: {gpu_memory:.1f}GB")
            return True, gpu_name, gpu_memory
        else:
            logger.error("âŒ PyTorch CUDAä¸å¯ç”¨")
            raise RuntimeError("GPUä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")
    except Exception as e:
        logger.error(f"âŒ GPUæ£€æŸ¥å¤±è´¥: {e}")
        raise RuntimeError(f"GPUæ£€æŸ¥å¤±è´¥: {e}")

def discover_models():
    """å‘ç°å¯ç”¨æ¨¡å‹"""
    model_paths = [
        "/runpod-volume/text_models/L3.2-8X4B.gguf",  # è¾ƒå°çš„æ¨¡å‹
        "/runpod-volume/text_models/L3.2-8X3B.gguf"   # è¾ƒå¤§çš„æ¨¡å‹
    ]
    
    available_models = []
    for path in model_paths:
        if os.path.exists(path):
            size = os.path.getsize(path) / (1024**3)  # å¤§å°(GB)
            available_models.append((path, size))
            logger.info(f"ğŸ“ å‘ç°æ¨¡å‹: {path} ({size:.1f}GB)")
        else:
            logger.warning(f"âš ï¸ æ¨¡å‹æœªæ‰¾åˆ°: {path}")
    
    return available_models

def load_gguf_model(model_path: str):
    """å¼ºåˆ¶GPUæ¨¡å¼åŠ è½½GGUFæ¨¡å‹"""
    try:
        from llama_cpp import Llama
        import llama_cpp
        
        logger.info(f"ğŸš€ llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        logger.info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
        
        # æ£€æŸ¥GPU
        gpu_available, gpu_name, gpu_memory = check_gpu()
        
        # å¼ºåˆ¶ä½¿ç”¨å…¨éƒ¨GPUå±‚
        logger.info(f"ğŸ¯ å¼ºåˆ¶GPUæ¨¡å¼: å…¨éƒ¨å±‚åˆ°GPU ({gpu_name})")
        
        # æ£€æŸ¥åˆå§‹GPUçŠ¶æ€
        check_gpu_usage()
        
        model = Llama(
            model_path=model_path,
            n_ctx=4096,           # æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£
            n_batch=1024,         # æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°
            n_gpu_layers=-1,      # å…¨éƒ¨å±‚åˆ°GPU
            verbose=False,        # å…³é—­è¯¦ç»†æ—¥å¿—å‡å°‘å™ªéŸ³
            use_mmap=True,
            use_mlock=False,
            n_threads=1,          # GPUæ¨¡å¼ä¸‹å‡å°‘CPUçº¿ç¨‹
        )
        
        logger.info("âœ… æ¨¡å‹GPUåŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥åŠ è½½åGPUçŠ¶æ€
        check_gpu_usage()
        
        return model, "gguf_gpu"
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global model, model_type, model_path
    
    logger.info("ğŸ”„ å¼€å§‹æ¨¡å‹åˆå§‹åŒ–...")
    
    # å‘ç°å¯ç”¨æ¨¡å‹
    available_models = discover_models()
    
    if not available_models:
        raise RuntimeError("æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹")
    
    # æŒ‰å¤§å°æ’åºæ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼‰
    available_models.sort(key=lambda x: x[1])
    
    # åŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
    model_path_candidate, size = available_models[0]
    logger.info(f"ğŸ¯ é€‰æ‹©æ¨¡å‹: {model_path_candidate} ({size:.1f}GB)")
    
    if model_path_candidate.endswith('.gguf'):
        loaded_model, loaded_type = load_gguf_model(model_path_candidate)
        
        if loaded_model:
            model = loaded_model
            model_type = loaded_type
            model_path = model_path_candidate
            logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {model_path}")
            return True
    
    raise RuntimeError("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")

def get_personality_prompt(personality: str) -> str:
    """è·å–AIäººæ ¼æç¤ºè¯"""
    personalities = {
        "default": "You are a helpful AI assistant. Provide clear, accurate, and helpful responses.",
        "creative": "You are a creative AI assistant. Think outside the box and provide imaginative ideas.",
        "academic": "You are an academic AI assistant. Provide well-researched, precise, and scholarly responses.",
        "friendly": "You are a friendly AI assistant. Be warm, approachable, and conversational in your responses.",
        "professional": "You are a professional AI assistant. Provide concise, practical, and business-oriented advice."
    }
    return personalities.get(personality, personalities["default"])

def format_llama_prompt(prompt: str, personality: str = "default") -> str:
    """æ ¼å¼åŒ–Llama 3.2æç¤ºè¯"""
    system_prompt = get_personality_prompt(personality)
    
    # ç›´æ¥æ„å»ºæç¤ºè¯ï¼Œä¸æ£€æŸ¥é‡å¤
    formatted_prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    
    return formatted_prompt

def generate_response(prompt: str, personality: str = "default") -> str:
    """ç”Ÿæˆå“åº”"""
    global model, model_type, model_path
    
    if model is None:
        raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
    
    try:
        # æ ¼å¼åŒ–æç¤ºè¯
        formatted_prompt = format_llama_prompt(prompt, personality)
        logger.info(f"ğŸ’­ ç”Ÿæˆå“åº” (äººæ ¼: {personality})")
        
        # æ£€æŸ¥ç”Ÿæˆå‰GPUçŠ¶æ€
        check_gpu_usage()
        
        # ç”Ÿæˆå‚æ•°
        generation_params = {
            "max_tokens": 512,
            "temperature": 0.7,
            "top_p": 0.9,
            "repeat_penalty": 1.1,
            "stop": ["<|eot_id|>", "<|end_of_text|>", "<|start_header_id|>"],
            "echo": False
        }
        
        # ç”Ÿæˆå“åº”
        start_time = time.time()
        output = model(formatted_prompt, **generation_params)
        elapsed = time.time() - start_time
        
        # æ£€æŸ¥ç”ŸæˆåGPUçŠ¶æ€
        check_gpu_usage()
        
        # æå–å“åº”æ–‡æœ¬
        if isinstance(output, dict) and "choices" in output and len(output["choices"]) > 0:
            response_text = output["choices"][0].get("text", "").strip()
        else:
            response_text = str(output).strip()
        
        if not response_text:
            response_text = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„å“åº”ã€‚"
        
        logger.info(f"âš¡ ç”Ÿæˆå®Œæˆ: {elapsed:.2f}ç§’, é•¿åº¦: {len(response_text)}")
        
        return response_text
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆå“åº”é”™è¯¯: {e}")
        return f"ç”Ÿæˆé”™è¯¯: {str(e)}"

def handler(event):
    """RunPod handlerå‡½æ•°"""
    try:
        logger.info("ğŸ¯ Handlerè°ƒç”¨")
        
        # æå–è¾“å…¥
        user_input = event.get("input", {})
        prompt = user_input.get("prompt", "")
        personality = user_input.get("personality", "default")
        
        if not prompt:
            return {
                "output": "è¯·æä¾›æœ‰æ•ˆçš„æç¤ºè¯",
                "status": "error"
            }
        
        logger.info(f"ğŸ“ æç¤ºè¯: {prompt[:50]}...")
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœæœªåˆå§‹åŒ–ï¼‰
        global model
        if model is None:
            logger.info("ğŸ”„ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            success = initialize_model()
            if not success:
                return {
                    "output": "æ¨¡å‹åˆå§‹åŒ–å¤±è´¥",
                    "status": "error"
                }
        
        # ç”Ÿæˆå“åº”
        response = generate_response(prompt, personality)
        
        # è¿”å›æ ‡å‡†æ ¼å¼
        result = {
            "output": response,
            "status": "success",
            "model_info": {
                "model_path": model_path,
                "model_type": model_type
            }
        }
        
        logger.info(f"âœ… å“åº”è¿”å›: {len(response)}å­—ç¬¦")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Handleré”™è¯¯: {e}")
        return {
            "output": f"ç³»ç»Ÿé”™è¯¯: {str(e)}",
            "status": "error"
        }

# å¯åŠ¨RunPod serverless
if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨GPUä¼˜åŒ–RunPod handler...")
    runpod.serverless.start({"handler": handler})