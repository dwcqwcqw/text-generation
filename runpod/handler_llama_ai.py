#!/usr/bin/env python3
"""
RunPod Handler - ä¿®å¤GPUä½¿ç”¨å’Œå‰ç«¯å“åº”é—®é¢˜
ä¸“ä¸ºL40 GPUä¼˜åŒ–ï¼Œ45GBæ˜¾å­˜
"""

import runpod
import os
import logging
import time
import subprocess
import json
from typing import Optional, Dict, Any, Tuple, List
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(filename)s :%(lineno)d  %(asctime)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
model = None
model_type = None
model_path = None

# å¼ºåˆ¶è®¾ç½®CUDAç¯å¢ƒå˜é‡
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['GGML_CUDA'] = '1'
    # LLAMA_CUBLASå·²å¼ƒç”¨ï¼Œä½¿ç”¨GGML_CUDA
os.environ['CMAKE_CUDA_ARCHITECTURES'] = '75;80;86;89'  # æ”¯æŒå¤šç§GPUæ¶æ„
os.environ['FORCE_CMAKE'] = '1'
os.environ['CMAKE_ARGS'] = '-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=75;80;86;89'

# å¼ºåˆ¶x86_64æ¶æ„ï¼Œé¿å…ARM64é—®é¢˜
os.environ['ARCHFLAGS'] = '-arch x86_64'
os.environ['CFLAGS'] = '-march=x86-64'
os.environ['CXXFLAGS'] = '-march=x86-64'

try:
    from llama_cpp import Llama
    try:
        import GPUtil
    except ImportError:
        logger.warning("GPUtilæœªå®‰è£…ï¼Œä½¿ç”¨nvidia-smiæ›¿ä»£")
        GPUtil = None
except ImportError as e:
    logging.error(f"å¯¼å…¥å¤±è´¥: {e}")
    raise

def check_gpu_usage():
    """æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ"""
    try:
        if GPUtil:
            # ä½¿ç”¨GPUtil
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                logger.info(f"ğŸ”¥ GPUçŠ¶æ€: åˆ©ç”¨ç‡{gpu.load*100:.0f}%, æ˜¾å­˜{gpu.memoryUsed/1024:.1f}/{gpu.memoryTotal/1024:.1f}GB, æ¸©åº¦{gpu.temperature}Â°C")
                return gpu.memoryTotal / 1024, gpu.memoryUsed / 1024
            else:
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPU")
                return None, None
        else:
            # ä½¿ç”¨nvidia-smiæ›¿ä»£
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                gpu_info = result.stdout.strip().split(', ')
                if len(gpu_info) >= 4:
                    util = gpu_info[0]
                    mem_used = float(gpu_info[1]) / 1024  # MB to GB
                    mem_total = float(gpu_info[2]) / 1024  # MB to GB
                    temp = gpu_info[3]
                    logger.info(f"ğŸ”¥ GPUçŠ¶æ€: åˆ©ç”¨ç‡{util}%, æ˜¾å­˜{mem_used:.1f}/{mem_total:.1f}GB, æ¸©åº¦{temp}Â°C")
                    return mem_total, mem_used
            logger.warning("âš ï¸ æ— æ³•è·å–GPUä¿¡æ¯")
            return None, None
    except Exception as e:
        logger.error(f"âŒ GPUæ£€æŸ¥å¤±è´¥: {e}")
        return None, None

def find_models() -> List[Tuple[str, float]]:
    """æŸ¥æ‰¾å¯ç”¨çš„GGUFæ¨¡å‹"""
    model_dir = Path("/runpod-volume/text_models")
    models = []
    
    if model_dir.exists():
        for model_file in model_dir.glob("*.gguf"):
            size_gb = model_file.stat().st_size / (1024**3)
            models.append((str(model_file), size_gb))
            logger.info(f"ğŸ“ å‘ç°æ¨¡å‹: {model_file} ({size_gb:.1f}GB)")
    
    # æŒ‰å¤§å°æ’åºï¼Œå°æ¨¡å‹ä¼˜å…ˆ
    models.sort(key=lambda x: x[1])
    return models

def load_gguf_model(model_path: str) -> Tuple[Llama, str]:
    """åŠ è½½GGUFæ¨¡å‹ï¼Œå¼ºåˆ¶GPUæ¨¡å¼"""
    try:
        logger.info(f"llama-cpp-pythonç‰ˆæœ¬: {Llama.__version__ if hasattr(Llama, '__version__') else 'æœªçŸ¥'}")
        logger.info(f"ğŸ“‚ å¼ºåˆ¶GPUæ¨¡å¼åŠ è½½: {model_path}")
        
        # æ£€æŸ¥GPUçŠ¶æ€
        mem_total, mem_used = check_gpu_usage()
        
        # å¼ºåˆ¶æ‰€æœ‰å±‚åˆ°GPUï¼Œä¸ç®¡GPUå¤§å°
        logger.info(f"ğŸ¯ å¼ºåˆ¶æ‰€æœ‰å±‚åˆ°GPU (n_gpu_layers=-1)")
        
        # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´é…ç½® - ä¿å®ˆè®¾ç½®é¿å…OOM
        if mem_total and mem_total > 40:  # RTX 4090ç­‰é«˜ç«¯GPU
            n_ctx = 32768      # å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
            n_batch = 1024     # ä¸­ç­‰æ‰¹å¤„ç†
        elif mem_total and mem_total > 20:  # L4 GPUç­‰
            n_ctx = 16384      # ä½¿ç”¨è¾ƒå°ä¸Šä¸‹æ–‡
            n_batch = 512      # å°æ‰¹å¤„ç†
        else:
            n_ctx = 8192       # ä½¿ç”¨æœ€å°ä¸Šä¸‹æ–‡
            n_batch = 256      # æœ€å°æ‰¹å¤„ç†
        
        logger.info(f"ğŸ”§ GPUé…ç½®: n_gpu_layers=-1 (å…¨éƒ¨), n_ctx={n_ctx}, n_batch={n_batch}")
        
        # å¼ºåˆ¶GPUæ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ä¼˜åŒ–
        try:
            model = Llama(
                model_path=model_path,
                n_ctx=n_ctx,              # ä¸Šä¸‹æ–‡é•¿åº¦
                n_batch=n_batch,          # æ‰¹å¤„ç†å¤§å°
                n_gpu_layers=-1,          # å¼ºåˆ¶æ‰€æœ‰å±‚åˆ°GPU
                verbose=True,             # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ä»¥æŸ¥çœ‹å±‚åˆ†é…
                n_threads=1,              # æœ€å°‘CPUçº¿ç¨‹ï¼Œä¸“æ³¨GPU
                use_mmap=True,            # ä½¿ç”¨å†…å­˜æ˜ å°„
                use_mlock=False,          # ä¸é”å®šå†…å­˜
                f16_kv=True,              # ä½¿ç”¨FP16 KVç¼“å­˜èŠ‚çœæ˜¾å­˜
                logits_all=False,         # ä¸è®¡ç®—æ‰€æœ‰logitsèŠ‚çœè®¡ç®—
                # å¼ºåˆ¶CUDAåç«¯
                main_gpu=0,               # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
                tensor_split=None,        # ä¸åˆ†å‰²å¼ é‡
                rope_scaling_type=None,   # ä¸ä½¿ç”¨ropeç¼©æ”¾
                rope_freq_base=0.0,       # ä½¿ç”¨é»˜è®¤é¢‘ç‡åŸºæ•°
                rope_freq_scale=0.0,      # ä½¿ç”¨é»˜è®¤é¢‘ç‡ç¼©æ”¾
            )
        except Exception as gpu_error:
            # å¦‚æœGPUåŠ è½½å¤±è´¥ï¼Œå°è¯•æ›´ä¿å®ˆçš„è®¾ç½®
            logger.warning(f"âš ï¸ GPUåŠ è½½å¤±è´¥ï¼Œå°è¯•æ›´ä¿å®ˆçš„è®¾ç½®: {gpu_error}")
            logger.info("ğŸ”„ å°è¯•å‡å°‘å†…å­˜ä½¿ç”¨...")
            
            model = Llama(
                model_path=model_path,
                n_ctx=4096,               # æœ€å°ä¸Šä¸‹æ–‡
                n_batch=128,              # æœ€å°æ‰¹å¤„ç†
                n_gpu_layers=-1,          # ä»ç„¶å°è¯•GPU
                verbose=True,
                n_threads=1,
                use_mmap=True,
                use_mlock=False,
                f16_kv=True,
                logits_all=False,
                main_gpu=0,
            )
        
        logger.info("âœ… æ¨¡å‹GPUåŠ è½½æˆåŠŸ")
        check_gpu_usage()  # æ˜¾ç¤ºåŠ è½½åçš„GPUçŠ¶æ€
        return model, "gguf"
        
    except Exception as e:
        logger.error(f"âŒ GGUFæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        logger.error("ğŸ’¡ æç¤ºï¼šå¦‚æœæ˜¯GPUå†…å­˜ä¸è¶³ï¼Œè¯·å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é‡å¯å®¹å™¨")
        raise e

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global model, model_type, model_path
    
    logger.info("ğŸ”„ å¼€å§‹æ¨¡å‹åˆå§‹åŒ–...")
    
    # æŸ¥æ‰¾æ¨¡å‹
    models = find_models()
    if not models:
        raise Exception("æœªæ‰¾åˆ°ä»»ä½•GGUFæ¨¡å‹æ–‡ä»¶")
    
    # å¦‚æœå·²ç»æŒ‡å®šäº†æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
    if model_path:
        logger.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹: {model_path}")
        # éªŒè¯æŒ‡å®šçš„æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            logger.error(f"âŒ æŒ‡å®šçš„æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
            # å›é€€åˆ°é»˜è®¤é€‰æ‹©
            selected_model = models[0]
            model_path = selected_model[0]
            model_size = selected_model[1]
            logger.info(f"ğŸ”„ å›é€€åˆ°é»˜è®¤æ¨¡å‹: {model_path} ({model_size:.1f}GB)")
        else:
            # è·å–æŒ‡å®šæ¨¡å‹çš„å¤§å°ä¿¡æ¯
            model_size = os.path.getsize(model_path) / (1024**3)  # GB
            logger.info(f"ğŸ“ æŒ‡å®šæ¨¡å‹å¤§å°: {model_size:.1f}GB")
            logger.info(f"âœ… ç¡®è®¤ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {os.path.basename(model_path)}")
    else:
        # æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œé€‰æ‹©æœ€å°çš„æ¨¡å‹ï¼ˆæ›´å¿«åŠ è½½ï¼‰
        selected_model = models[0]
        model_path = selected_model[0]
        model_size = selected_model[1]
        logger.info(f"ğŸ¯ è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {model_path} ({model_size:.1f}GB)")
    
    # åŠ è½½æ¨¡å‹
    model, model_type = load_gguf_model(model_path)
    
    logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {model_path}")
    return True

def format_prompt(prompt: str, persona: str = "default", history: list = None) -> str:
    """æ ¼å¼åŒ–æç¤ºè¯ï¼Œé¿å…é‡å¤BOSæ ‡è®°ï¼Œæ”¯æŒå¯¹è¯å†å²"""
    
    # æ¸…ç†è¾“å…¥æç¤ºè¯
    prompt = str(prompt).strip()
    if not prompt:
        prompt = "Hello"
    
    # æ ¹æ®äººæ ¼è®¾ç½®ç³»ç»Ÿæç¤ºè¯ - æ·»åŠ è¡¨æƒ…å’Œç®€æ´å›å¤è¦æ±‚
    system_prompts = {
        "default": "You are a helpful, intelligent AI assistant for general conversations. å›å¤å°½é‡çš„è¨€ç®€æ„èµ…ï¼Œå¹¶ä¸”çµæ´»çš„ä½¿ç”¨å„ç§è¡¨æƒ…ç¬¦å·æ¥å¢åŠ äº²å’ŒåŠ›ã€‚Keep responses concise and use emojis appropriately.",
        "creative": "You are a creative AI assistant specialized in creative writing, storytelling, and fiction. å›å¤å°½é‡çš„è¨€ç®€æ„èµ…ï¼Œå¹¶ä¸”çµæ´»çš„ä½¿ç”¨å„ç§è¡¨æƒ…ç¬¦å·ã€‚Be creative and expressive with emojis.",
        "professional": "You are a professional AI assistant providing formal, structured responses for business and analysis. å›å¤å°½é‡çš„è¨€ç®€æ„èµ…ï¼Œé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·ã€‚Maintain professionalism while being concise.",
        "casual": "You are a friendly, relaxed AI assistant with a conversational style. å›å¤å°½é‡çš„è¨€ç®€æ„èµ…ï¼Œå¹¶ä¸”çµæ´»çš„ä½¿ç”¨å„ç§è¡¨æƒ…ç¬¦å·ã€‚Be casual and use lots of emojis!",
        "technical": "You are a technical AI assistant with expertise in programming, technology, and engineering. å›å¤å°½é‡çš„è¨€ç®€æ„èµ…ï¼Œå¹¶ä¸”é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·ã€‚Be precise and use relevant emojis.",
        "chinese": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡AIåŠ©æ‰‹ï¼Œç†è§£ä¸­æ–‡æ–‡åŒ–èƒŒæ™¯ã€‚å›å¤å°½é‡çš„è¨€ç®€æ„èµ…ï¼Œå¹¶ä¸”çµæ´»çš„ä½¿ç”¨å„ç§è¡¨æƒ…ç¬¦å·æ¥å¢åŠ äº²å’ŒåŠ›ã€‚"
    }
    
    system_prompt = system_prompts.get(persona, system_prompts["default"])
    
    # æ„å»ºå¯¹è¯å†å²
    conversation = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"
    
    # æ·»åŠ å†å²å¯¹è¯
    if history:
        for msg in history:
            if isinstance(msg, dict):
                role = msg.get('role', 'user')
                content = str(msg.get('content', '')).strip()
                
                # è·³è¿‡ç©ºå†…å®¹æˆ–æ— æ•ˆå†…å®¹
                if not content or content == '[object Object]':
                    continue
                    
                if role in ['user', 'assistant']:
                    conversation += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}<|eot_id|>"
    
    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    conversation += f"<|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    
    return conversation

def generate_response(prompt: str, persona: str = "default", history: list = None, stream: bool = False) -> str:
    """ç”ŸæˆAIå“åº”ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œå¯¹è¯å†å²"""
    global model
    
    if not model:
        raise Exception("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    logger.info(f"ğŸ’­ ç”Ÿæˆå“åº” (äººæ ¼: {persona}, æµå¼: {stream})")
    logger.info(f"ğŸ“ åŸå§‹è¾“å…¥: '{prompt}'")
    if history:
        logger.info(f"ğŸ“š å†å²è®°å½•æ•°é‡: {len(history)}")
    
    # æ¸…ç†æç¤ºè¯å¹¶åŒ…å«å†å²è®°å½•
    formatted_prompt = format_prompt(prompt, persona, history)
    logger.info(f"ğŸ“ æ ¼å¼åŒ–åé•¿åº¦: {len(formatted_prompt)}")
    
    # æ£€æŸ¥ç”Ÿæˆå‰GPUçŠ¶æ€
    check_gpu_usage()
    
    start_time = time.time()
    
    try:
        # ç”Ÿæˆå“åº” - å¢åŠ max_tokensä»¥æ”¯æŒæ›´å®Œæ•´çš„å›å¤
        response = model(
            formatted_prompt,
            max_tokens=512,       # å¢åŠ tokenæ•°é‡ä»¥æ”¯æŒå®Œæ•´å›å¤
            temperature=0.7,
            top_p=0.9,
            top_k=40,
            repeat_penalty=1.1,
            stop=["<|eot_id|>", "<|end_of_text|>", "\n\n---", "<|start_header_id|>"],
            echo=False,           # ä¸å›æ˜¾è¾“å…¥
            stream=stream         # æ”¯æŒæµå¼è¾“å‡º
        )
        
        # å¤„ç†æµå¼å“åº”
        if stream:
            # å¦‚æœæ˜¯æµå¼å“åº”ï¼Œè¿”å›ç”Ÿæˆå™¨
            def stream_generator():
                full_response = ""
                for chunk in response:
                    if isinstance(chunk, dict) and 'choices' in chunk:
                        if len(chunk['choices']) > 0:
                            delta = chunk['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                full_response += content
                                yield content
                    else:
                        content = str(chunk).strip()
                        if content:
                            full_response += content
                            yield content
                
                # è®°å½•å®Œæ•´å“åº”
                generation_time = time.time() - start_time
                check_gpu_usage()
                logger.info(f"âš¡ æµå¼ç”Ÿæˆå®Œæˆ: {generation_time:.2f}ç§’")
                logger.info(f"ğŸ“¤ å®Œæ•´å“åº”: '{full_response}' (é•¿åº¦: {len(full_response)})")
            
            return stream_generator()
        else:
            # éæµå¼å“åº”
            response_text = ""
            if isinstance(response, dict) and 'choices' in response:
                if len(response['choices']) > 0:
                    response_text = response['choices'][0].get('text', '').strip()
            else:
                response_text = str(response).strip()
            
            generation_time = time.time() - start_time
            
            # æ£€æŸ¥ç”ŸæˆåGPUçŠ¶æ€
            check_gpu_usage()
            
            logger.info(f"âš¡ ç”Ÿæˆå®Œæˆ: {generation_time:.2f}ç§’")
            logger.info(f"ğŸ“¤ åŸå§‹å“åº”: '{response_text}'")
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            if response_text:
                # ç§»é™¤å¯èƒ½çš„æ ¼å¼æ ‡è®°
                response_text = response_text.replace('<|eot_id|>', '').replace('<|end_of_text|>', '').strip()
                logger.info(f"ğŸ“¤ æ¸…ç†åå“åº”: '{response_text}' (é•¿åº¦: {len(response_text)})")
            
            # å¦‚æœå“åº”ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯
            if not response_text:
                response_text = "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œä½†ç›®å‰æ— æ³•æä¾›å…·ä½“å›ç­”ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚ğŸ˜Š"
                logger.warning("âš ï¸ å“åº”ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ¶ˆæ¯")
            
            return response_text
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆå“åº”å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯: {str(e)} ğŸ˜”"

def handler(event):
    """RunPodå¤„ç†å‡½æ•° - æ”¯æŒæµå¼å“åº”å’Œå¯¹è¯å†å²"""
    global model, model_path
    
    try:
        logger.info("ğŸ¯ Handlerè°ƒç”¨")
        logger.info(f"ğŸ“¥ å®Œæ•´äº‹ä»¶: {json.dumps(event, indent=2, ensure_ascii=False)}")
        
        # è·å–è¾“å…¥
        input_data = event.get("input", {})
        prompt = input_data.get("prompt", "").strip()
        persona = input_data.get("system_template", input_data.get("persona", "default"))
        requested_model_path = input_data.get("model_path", "")  # å‰ç«¯æŒ‡å®šçš„æ¨¡å‹è·¯å¾„
        history = input_data.get("history", [])  # å¯¹è¯å†å²
        stream = input_data.get("stream", False)  # æ˜¯å¦å¯ç”¨æµå¼å“åº”
        
        logger.info(f"ğŸ“ æå–çš„æç¤ºè¯: '{prompt}'")
        logger.info(f"ğŸ‘¤ äººæ ¼è®¾ç½®: '{persona}'")
        logger.info(f"ğŸ¯ è¯·æ±‚çš„æ¨¡å‹: '{requested_model_path}'")
        logger.info(f"ğŸŒŠ æµå¼å“åº”: {stream}")
        
        if not prompt:
            # RunPodæ ¼å¼çš„é”™è¯¯å“åº”
            error_result = {
                "status": "FAILED",
                "error": "è¯·æä¾›æœ‰æ•ˆçš„æç¤ºè¯",
                "output": None
            }
            logger.error(f"âŒ æ— æ•ˆè¾“å…¥ï¼Œè¿”å›: {json.dumps(error_result, ensure_ascii=False)}")
            return error_result
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢æ¨¡å‹
        if requested_model_path and requested_model_path != model_path:
            logger.info(f"ğŸ”„ éœ€è¦åˆ‡æ¢æ¨¡å‹: {model_path} -> {requested_model_path}")
            model = None  # é‡ç½®æ¨¡å‹ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½
            model_path = requested_model_path
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not model:
            logger.info("ğŸ”„ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            initialize_model()
        
        # ç”Ÿæˆå“åº”ï¼ŒåŒ…å«å†å²è®°å½•
        response = generate_response(prompt, persona, history, stream)
        
        # å¤„ç†æµå¼å“åº”
        if stream and hasattr(response, '__iter__'):
            # å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†æ‰€æœ‰chunkså¹¶è¿”å›å®Œæ•´å“åº”
            # å› ä¸ºRunPod serverlessä¸ç›´æ¥æ”¯æŒæµå¼å“åº”
            full_response = ""
            for chunk in response:
                full_response += chunk
            response = full_response
        
        # è¿”å›RunPodæ ‡å‡†æ ¼å¼
        result = {
            "status": "COMPLETED",  # å‰ç«¯æœŸæœ›çš„çŠ¶æ€
            "output": response,     # å‰ç«¯æœŸæœ›çš„è¾“å‡ºå­—æ®µ
            "model_info": f"æ¨¡å‹: {os.path.basename(model_path) if model_path else 'unknown'}",
            "stream": stream        # æ ‡è®°æ˜¯å¦ä¸ºæµå¼å“åº”
        }
        
        logger.info(f"âœ… æœ€ç»ˆè¿”å›ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Handleré”™è¯¯: {e}")
        # RunPodæ ¼å¼çš„é”™è¯¯å“åº”
        error_result = {
            "status": "FAILED",
            "error": f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)} ğŸ˜”",
            "output": None
        }
        logger.error(f"âŒ é”™è¯¯è¿”å›: {json.dumps(error_result, ensure_ascii=False)}")
        return error_result

if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨GPUä¼˜åŒ–RunPod handler...")
    
    # å¯åŠ¨æ—¶æ£€æŸ¥GPU
    check_gpu_usage()
    
    # å¯åŠ¨RunPodæœåŠ¡
    runpod.serverless.start({"handler": handler})