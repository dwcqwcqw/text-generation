#!/usr/bin/env python3
"""
RunPod Handler - å½»åº•ä¿®å¤GPUä½¿ç”¨é—®é¢˜
ä¸“ä¸ºL40 GPUä¼˜åŒ–ï¼Œ45GBæ˜¾å­˜
"""

import runpod
import os
import logging
import time
import subprocess
import json
from typing import Optional, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
model = None
model_type = None
model_path = None

# å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡ - ç¡®ä¿CUDAå¯ç”¨
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['LLAMA_CUBLAS'] = '1'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

def check_gpu_usage():
    """æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(', ')
            if len(gpu_info) >= 4:
                util = gpu_info[0]
                mem_used = float(gpu_info[1]) / 1024  # MB to GB
                mem_total = float(gpu_info[2]) / 1024  # MB to GB
                temp = gpu_info[3]
                logger.info(f"ğŸ”¥ GPUçŠ¶æ€: åˆ©ç”¨ç‡{util}%, æ˜¾å­˜{mem_used:.1f}/{mem_total:.1f}GB, æ¸©åº¦{temp}Â°C")
                return util, mem_used, mem_total, temp
    except Exception as e:
        logger.error(f"GPUçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
    return None, None, None, None

def find_models():
    """æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶"""
    model_dir = "/runpod-volume/text_models"
    models = []
    
    if os.path.exists(model_dir):
        for file in os.listdir(model_dir):
            if file.endswith('.gguf'):
                file_path = os.path.join(model_dir, file)
                file_size = os.path.getsize(file_path) / (1024**3)  # GB
                models.append((file_path, file_size))
                logger.info(f"ğŸ“ å‘ç°æ¨¡å‹: {file_path} ({file_size:.1f}GB)")
    
    return sorted(models, key=lambda x: x[1])  # æŒ‰å¤§å°æ’åº

def load_gguf_model(model_path: str):
    """å¼ºåˆ¶GPUæ¨¡å¼åŠ è½½GGUFæ¨¡å‹"""
    try:
        from llama_cpp import Llama
        import llama_cpp
        
        logger.info(f"ğŸš€ llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        logger.info(f"ğŸ“‚ å¼ºåˆ¶GPUæ¨¡å¼åŠ è½½: {model_path}")
        
        # æ£€æŸ¥GPU
        util, mem_used, mem_total, temp = check_gpu_usage()
        if mem_total and mem_total > 40:  # L40 GPU
            logger.info(f"ğŸ¯ æ£€æµ‹åˆ°L40 GPU ({mem_total:.1f}GB)ï¼Œä½¿ç”¨å…¨éƒ¨GPUå±‚")
            n_gpu_layers = -1  # å…¨éƒ¨å±‚åˆ°GPU
        else:
            logger.info(f"ğŸ¯ ä½¿ç”¨é»˜è®¤GPUé…ç½®")
            n_gpu_layers = 32  # å¤§éƒ¨åˆ†å±‚åˆ°GPU
        
        # å¼ºåˆ¶GPUæ¨¡å¼å‚æ•°
        model = Llama(
            model_path=model_path,
            n_ctx=4096,           # å¤§ä¸Šä¸‹æ–‡çª—å£
            n_batch=1024,         # å¤§æ‰¹å¤„ç†
            n_gpu_layers=n_gpu_layers,  # å¼ºåˆ¶GPUå±‚æ•°
            verbose=False,        # å…³é—­è¯¦ç»†æ—¥å¿—
            n_threads=1,          # æœ€å°‘CPUçº¿ç¨‹
            use_mmap=True,
            use_mlock=False,
            rope_scaling_type=1,
            rope_freq_base=500000.0,
        )
        
        logger.info("âœ… æ¨¡å‹GPUåŠ è½½æˆåŠŸ")
        check_gpu_usage()  # æ£€æŸ¥åŠ è½½åçš„GPUçŠ¶æ€
        return model, "gguf"
        
    except Exception as e:
        logger.error(f"âŒ GGUFæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global model, model_type, model_path
    
    logger.info("ğŸ”„ å¼€å§‹æ¨¡å‹åˆå§‹åŒ–...")
    
    # æŸ¥æ‰¾æ¨¡å‹
    models = find_models()
    if not models:
        raise Exception("æœªæ‰¾åˆ°ä»»ä½•GGUFæ¨¡å‹æ–‡ä»¶")
    
    # é€‰æ‹©æœ€å°çš„æ¨¡å‹ï¼ˆæ›´å¿«åŠ è½½ï¼‰
    selected_model = models[0]
    model_path = selected_model[0]
    model_size = selected_model[1]
    
    logger.info(f"ğŸ¯ é€‰æ‹©æ¨¡å‹: {model_path} ({model_size:.1f}GB)")
    
    # åŠ è½½æ¨¡å‹
    model, model_type = load_gguf_model(model_path)
    
    logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {model_path}")
    return True

def clean_prompt(prompt: str) -> str:
    """æ¸…ç†æç¤ºè¯ï¼Œé¿å…é‡å¤æ ‡è®°"""
    # ç§»é™¤å¯èƒ½çš„é‡å¤æ ‡è®°
    prompt = prompt.strip()
    
    # å¦‚æœå·²ç»æœ‰å¼€å§‹æ ‡è®°ï¼Œä¸è¦é‡å¤æ·»åŠ 
    if prompt.startswith('<|begin_of_text|>'):
        return prompt
    
    # æ·»åŠ æ ‡å‡†æ ¼å¼
    return f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

def generate_response(prompt: str, persona: str = "default") -> str:
    """ç”ŸæˆAIå“åº”"""
    global model
    
    if not model:
        raise Exception("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    logger.info(f"ğŸ’­ ç”Ÿæˆå“åº” (äººæ ¼: {persona})")
    
    # æ¸…ç†æç¤ºè¯
    formatted_prompt = clean_prompt(prompt)
    logger.info(f"ğŸ“ æ ¼å¼åŒ–æç¤ºè¯é•¿åº¦: {len(formatted_prompt)}")
    
    # æ£€æŸ¥ç”Ÿæˆå‰GPUçŠ¶æ€
    check_gpu_usage()
    
    start_time = time.time()
    
    try:
        # ç”Ÿæˆå“åº”
        response = model(
            formatted_prompt,
            max_tokens=512,
            temperature=0.7,
            top_p=0.9,
            top_k=40,
            repeat_penalty=1.1,
            stop=["<|eot_id|>", "<|end_of_text|>", "\n\n---"],
            echo=False,  # ä¸å›æ˜¾è¾“å…¥
            stream=False
        )
        
        # æå–å“åº”æ–‡æœ¬
        if isinstance(response, dict) and 'choices' in response:
            response_text = response['choices'][0]['text'].strip()
        else:
            response_text = str(response).strip()
        
        generation_time = time.time() - start_time
        
        # æ£€æŸ¥ç”ŸæˆåGPUçŠ¶æ€
        check_gpu_usage()
        
        logger.info(f"âš¡ ç”Ÿæˆå®Œæˆ: {generation_time:.2f}ç§’, é•¿åº¦: {len(response_text)}")
        
        # å¦‚æœå“åº”ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯
        if not response_text:
            response_text = "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œä½†ç›®å‰æ— æ³•æä¾›å…·ä½“å›ç­”ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"
        
        return response_text
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆå“åº”å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

def handler(event):
    """RunPodå¤„ç†å‡½æ•°"""
    global model
    
    try:
        logger.info("ğŸ¯ Handlerè°ƒç”¨")
        
        # è·å–è¾“å…¥
        input_data = event.get("input", {})
        prompt = input_data.get("prompt", "").strip()
        persona = input_data.get("persona", "default")
        
        if not prompt:
            return {
                "output": "è¯·æä¾›æœ‰æ•ˆçš„æç¤ºè¯",
                "status": "error",
                "model_info": f"æ¨¡å‹æœªä½¿ç”¨ - æ— æ•ˆè¾“å…¥"
            }
        
        logger.info(f"ğŸ“ æç¤ºè¯: {prompt[:50]}...")
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not model:
            logger.info("ğŸ”„ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            initialize_model()
        
        # ç”Ÿæˆå“åº”
        response = generate_response(prompt, persona)
        
        # è¿”å›æ ‡å‡†æ ¼å¼
        result = {
            "output": response,
            "status": "success",
            "model_info": f"æ¨¡å‹: {os.path.basename(model_path) if model_path else 'unknown'}"
        }
        
        logger.info(f"âœ… å“åº”è¿”å›: {len(response)}å­—ç¬¦")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Handleré”™è¯¯: {e}")
        return {
            "output": f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}",
            "status": "error",
            "model_info": "é”™è¯¯çŠ¶æ€"
        }

if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨GPUä¼˜åŒ–RunPod handler...")
    
    # å¯åŠ¨æ—¶æ£€æŸ¥GPU
    check_gpu_usage()
    
    # å¯åŠ¨RunPodæœåŠ¡
    runpod.serverless.start({"handler": handler})