#!/usr/bin/env python3
"""
RunPod Handler with Real AI - Llama 3.2 MOE
æ”¯æŒçœŸæ­£çš„AIå¯¹è¯ï¼Œä¸å†æ˜¯echoæ¨¡å¼
"""

import runpod
import os
import sys
import subprocess
import json
import time
import threading
import requests
from typing import Dict, Any, Optional

# å°è¯•å¯¼å…¥transformerså’Œtorchç”¨äºç›´æ¥æ¨¡å‹åŠ è½½
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    TRANSFORMERS_AVAILABLE = True
    print("âœ… Transformers and PyTorch available for direct model loading")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"âš ï¸ Transformers not available: {e}")

# å°è¯•å¯¼å…¥llama-cpp-pythonç”¨äºGGUFæ¨¡å‹
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
    print("âœ… llama-cpp-python available for GGUF model loading")
except ImportError as e:
    LLAMA_CPP_AVAILABLE = False
    print(f"âš ï¸ llama-cpp-python not available: {e}")

def get_gpu_info():
    """è·å–GPUä½¿ç”¨æƒ…å†µ"""
    try:
        result = subprocess.run([
            'nvidia-smi', 
            '--query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw',
            '--format=csv,noheader,nounits'
        ], capture_output=True, text=True, timeout=5)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            gpu_info = []
            
            for line in lines:
                if line.strip():
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 7:
                        gpu_info.append({
                            'index': parts[0],
                            'name': parts[1],
                            'memory_used_mb': parts[2],
                            'memory_total_mb': parts[3],
                            'utilization_percent': parts[4],
                            'temperature_c': parts[5],
                            'power_draw_w': parts[6]
                        })
            
            return gpu_info
        else:
            return [{'error': 'nvidia-smi command failed'}]
            
    except Exception as e:
        return [{'error': f'GPU info error: {str(e)}'}]

def log_gpu_status():
    """è®°å½•GPUçŠ¶æ€åˆ°æ—¥å¿—"""
    gpu_info = get_gpu_info()
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    
    print(f"\n=== GPU Status at {timestamp} ===")
    for i, gpu in enumerate(gpu_info):
        if 'error' in gpu:
            print(f"GPU {i}: {gpu['error']}")
        else:
            memory_used = float(gpu['memory_used_mb']) if gpu['memory_used_mb'] != 'N/A' else 0
            memory_total = float(gpu['memory_total_mb']) if gpu['memory_total_mb'] != 'N/A' else 1
            memory_percent = (memory_used / memory_total * 100) if memory_total > 0 else 0
            
            print(f"GPU {gpu['index']} ({gpu['name']}):")
            print(f"  Memory: {gpu['memory_used_mb']}MB / {gpu['memory_total_mb']}MB ({memory_percent:.1f}%)")
            print(f"  Utilization: {gpu['utilization_percent']}%")
            print(f"  Temperature: {gpu['temperature_c']}Â°C")
            print(f"  Power: {gpu['power_draw_w']}W")
    print("=" * 50)

def start_gpu_monitoring():
    """å¯åŠ¨GPUç›‘æ§çº¿ç¨‹"""
    def monitor_loop():
        while True:
            log_gpu_status()
            time.sleep(30)
    
    monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
    monitor_thread.start()
    print("ğŸ–¥ï¸ GPUç›‘æ§å·²å¯åŠ¨ï¼Œæ¯30ç§’è®°å½•ä¸€æ¬¡çŠ¶æ€")

def find_model_files():
    """åœ¨å¸¸è§è·¯å¾„ä¸­æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶"""
    print("ğŸ” Searching for model files in volume...")
    
    # å¸¸è§çš„æ¨¡å‹å­˜å‚¨è·¯å¾„
    search_paths = [
        "/runpod-volume",
        "/workspace", 
        "/models",
        "/app/models",
        "/data",
        "/storage",
        ".",
        os.path.expanduser("~")
    ]
    
    model_files = []
    
    for base_path in search_paths:
        if os.path.exists(base_path):
            print(f"ğŸ“ Checking {base_path}...")
            try:
                for root, dirs, files in os.walk(base_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        file_lower = file.lower()
                        
                        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
                        if (file_lower.endswith('.gguf') or 
                            file_lower.endswith('.bin') or
                            file_lower.endswith('.safetensors') or
                            'pytorch_model' in file_lower or
                            'model.safetensors' in file_lower):
                            
                            size_mb = os.path.getsize(file_path) / (1024 * 1024)
                            model_files.append({
                                'path': file_path,
                                'name': file,
                                'size_mb': size_mb,
                                'type': 'gguf' if file_lower.endswith('.gguf') else 'transformers'
                            })
                            print(f"  ğŸ“„ Found: {file} ({size_mb:.1f}MB)")
                            
                        # æŸ¥æ‰¾æ¨¡å‹ç›®å½•ï¼ˆåŒ…å«config.jsonçš„ç›®å½•ï¼‰
                        if file == 'config.json':
                            model_dir = root
                            try:
                                with open(file_path, 'r') as f:
                                    config = json.load(f)
                                    if 'model_type' in config:
                                        model_files.append({
                                            'path': model_dir,
                                            'name': os.path.basename(model_dir),
                                            'size_mb': sum(os.path.getsize(os.path.join(model_dir, f)) 
                                                         for f in os.listdir(model_dir) 
                                                         if os.path.isfile(os.path.join(model_dir, f))) / (1024 * 1024),
                                            'type': 'transformers',
                                            'model_type': config.get('model_type', 'unknown')
                                        })
                                        print(f"  ğŸ“ Found model dir: {os.path.basename(model_dir)} ({config.get('model_type', 'unknown')})")
                            except:
                                pass
                                
            except PermissionError:
                print(f"  âŒ Permission denied accessing {base_path}")
            except Exception as e:
                print(f"  âš ï¸ Error scanning {base_path}: {e}")
        else:
            print(f"  âŒ Path {base_path} does not exist")
    
    # æŒ‰å¤§å°æ’åºï¼Œå¤§æ¨¡å‹ä¼˜å…ˆ
    model_files.sort(key=lambda x: x['size_mb'], reverse=True)
    
    print(f"\nğŸ“Š Found {len(model_files)} potential model files:")
    for i, model in enumerate(model_files[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  {i+1}. {model['name']} ({model['size_mb']:.1f}MB, {model['type']})")
    
    return model_files

# å…¨å±€å˜é‡å­˜å‚¨åŠ è½½çš„æ¨¡å‹
loaded_model = None
loaded_tokenizer = None
model_type = None

# AIç³»ç»Ÿè®¾å®šæ¨¡ç‰ˆ
SYSTEM_TEMPLATES = {
    "default": """You are a helpful, intelligent AI assistant. You provide accurate, thoughtful, and detailed responses to user questions. You are knowledgeable across many topics and can engage in meaningful conversations.""",
    
    "creative": """You are a creative AI assistant with exceptional storytelling abilities. You excel at creative writing, fiction, roleplay, and imaginative scenarios. You write with vivid prose, engaging dialogue, and compelling narratives. You can adapt to any genre or style requested.""",
    
    "professional": """You are a professional AI assistant focused on providing clear, accurate, and well-structured responses. You maintain a formal tone while being helpful and informative. You excel at analysis, problem-solving, and providing detailed explanations.""",
    
    "casual": """You are a friendly, casual AI assistant. You communicate in a relaxed, conversational tone while still being helpful and informative. You can engage in both serious discussions and light-hearted conversations.""",
    
    "technical": """You are a technical AI assistant with deep expertise in programming, technology, and engineering. You provide precise, detailed technical explanations and can help with coding, system design, and troubleshooting.""",
    
    "chinese": """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„ä¸­æ–‡AIåŠ©æ‰‹ã€‚ä½ èƒ½å¤Ÿç”¨æµåˆ©çš„ä¸­æ–‡è¿›è¡Œå¯¹è¯ï¼Œç†è§£ä¸­æ–‡æ–‡åŒ–èƒŒæ™¯ï¼Œå¹¶æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚ä½ æ—¢å¯ä»¥è¿›è¡Œæ­£å¼çš„è®¨è®ºï¼Œä¹Ÿå¯ä»¥è¿›è¡Œè½»æ¾çš„èŠå¤©ã€‚"""
}

def format_llama3_prompt(system_prompt: str, user_message: str, conversation_history: list = None) -> str:
    """
    æ ¼å¼åŒ–Llama 3.2çš„å¯¹è¯æ¨¡ç‰ˆ
    
    Args:
        system_prompt: ç³»ç»Ÿæç¤º
        user_message: ç”¨æˆ·æ¶ˆæ¯
        conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
    
    Returns:
        æ ¼å¼åŒ–çš„promptå­—ç¬¦ä¸²
    """
    
    # Llama 3.2 ä½¿ç”¨çš„å¯¹è¯æ ¼å¼
    formatted_prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"
    
    # æ·»åŠ å¯¹è¯å†å²
    if conversation_history:
        for msg in conversation_history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            formatted_prompt += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}<|eot_id|>"
    
    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    formatted_prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    
    return formatted_prompt

def load_model(model_info):
    """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
    global loaded_model, loaded_tokenizer, model_type
    
    print(f"ğŸš€ Loading model: {model_info['name']}")
    print(f"ğŸ“ Path: {model_info['path']}")
    print(f"ğŸ“ Size: {model_info['size_mb']:.1f}MB")
    print(f"ğŸ”§ Type: {model_info['type']}")
    
    try:
        if model_info['type'] == 'gguf' and LLAMA_CPP_AVAILABLE:
            print("ğŸ¦™ Loading GGUF model with llama-cpp-python...")
            
            # æ£€æµ‹GPU
            gpu_count = torch.cuda.device_count() if 'torch' in globals() else 0
            print(f"ğŸ–¥ï¸ Detected {gpu_count} GPU(s)")
            
            loaded_model = Llama(
                model_path=model_info['path'],
                n_ctx=4096,  # ä¸Šä¸‹æ–‡é•¿åº¦
                n_gpu_layers=-1 if gpu_count > 0 else 0,  # ä½¿ç”¨GPUå±‚æ•°
                verbose=True
            )
            model_type = 'gguf'
            print("âœ… GGUF model loaded successfully")
            return True
            
        elif model_info['type'] == 'transformers' and TRANSFORMERS_AVAILABLE:
            print("ğŸ¤— Loading Transformers model...")
            
            # æ£€æµ‹GPU
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ–¥ï¸ Using device: {device}")
            
            # åŠ è½½tokenizer
            print("ğŸ“ Loading tokenizer...")
            loaded_tokenizer = AutoTokenizer.from_pretrained(
                model_info['path'],
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            print("ğŸ§  Loading model...")
            loaded_model = AutoModelForCausalLM.from_pretrained(
                model_info['path'],
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )
            
            if device == "cpu":
                loaded_model = loaded_model.to(device)
                
            model_type = 'transformers'
            print("âœ… Transformers model loaded successfully")
            return True
            
        else:
            print(f"âŒ Cannot load {model_info['type']} model - required libraries not available")
            return False
            
    except Exception as e:
        print(f"ğŸ’¥ Error loading model: {str(e)}")
        loaded_model = None
        loaded_tokenizer = None
        model_type = None
        return False

def generate_with_loaded_model(prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
    """ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹ç”Ÿæˆå›å¤"""
    global loaded_model, loaded_tokenizer, model_type
    
    if loaded_model is None:
        return "MODEL_NOT_LOADED"
    
    try:
        if model_type == 'gguf':
            print("ğŸ¦™ Generating with GGUF model...")
            
            response = loaded_model(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                top_k=40,
                repeat_penalty=1.1,
                stop=["<|eot_id|>", "<|end_of_text|>"]
            )
            
            return response['choices'][0]['text'].strip()
            
        elif model_type == 'transformers':
            print("ğŸ¤— Generating with Transformers model...")
            
            # ç¼–ç è¾“å…¥
            inputs = loaded_tokenizer.encode(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = inputs.to("cuda")
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = loaded_model.generate(
                    inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.9,
                    top_k=40,
                    do_sample=True,
                    pad_token_id=loaded_tokenizer.eos_token_id,
                    eos_token_id=loaded_tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            response = loaded_tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            return response.strip()
            
        else:
            return "UNKNOWN_MODEL_TYPE"
            
    except Exception as e:
        print(f"ğŸ’¥ Error generating with model: {str(e)}")
        return f"GENERATION_ERROR: {str(e)}"

def call_local_llm(prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
    """
    è°ƒç”¨æœ¬åœ°LLM API (å‡è®¾ä½¿ç”¨llama.cpp serveræˆ–ç±»ä¼¼æœåŠ¡)
    
    Args:
        prompt: æ ¼å¼åŒ–çš„prompt
        max_tokens: æœ€å¤§tokenæ•°
        temperature: æ¸©åº¦å‚æ•°
    
    Returns:
        AIç”Ÿæˆçš„å›å¤
    """
    
    # å¯èƒ½çš„LLMæœåŠ¡ç«¯ç‚¹åˆ—è¡¨
    llm_endpoints = [
        {
            "name": "llama.cpp server",
            "url": "http://localhost:8080/completion",
            "format": "llamacpp"
        },
        {
            "name": "text-generation-webui",
            "url": "http://localhost:5000/api/v1/generate",
            "format": "textgen"
        },
        {
            "name": "vLLM server",
            "url": "http://localhost:8000/v1/completions",
            "format": "openai"
        },
        {
            "name": "Ollama",
            "url": "http://localhost:11434/api/generate",
            "format": "ollama"
        }
    ]
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹
    if loaded_model is not None:
        print(f"ğŸ¯ Using pre-loaded model ({model_type})")
        result = generate_with_loaded_model(prompt, max_tokens, temperature)
        if result not in ["MODEL_NOT_LOADED", "UNKNOWN_MODEL_TYPE"] and not result.startswith("GENERATION_ERROR"):
            return result
        else:
            print(f"âš ï¸ Pre-loaded model failed: {result}")
    
    print(f"ğŸ” Attempting to connect to local LLM services...")
    
    for endpoint in llm_endpoints:
        try:
            print(f"ğŸ“¡ Trying {endpoint['name']} at {endpoint['url']}")
            
            # æ ¹æ®ä¸åŒæœåŠ¡æ ¼å¼åŒ–è¯·æ±‚
            if endpoint['format'] == 'llamacpp':
                payload = {
                    "prompt": prompt,
                    "n_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repeat_penalty": 1.1,
                    "stop": ["<|eot_id|>", "<|end_of_text|>"]
                }
            elif endpoint['format'] == 'textgen':
                payload = {
                    "prompt": prompt,
                    "max_new_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repetition_penalty": 1.1,
                    "stopping_strings": ["<|eot_id|>", "<|end_of_text|>"]
                }
            elif endpoint['format'] == 'openai':
                payload = {
                    "model": "llama-3.2-8x3b-moe",
                    "prompt": prompt,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "stop": ["<|eot_id|>", "<|end_of_text|>"]
                }
            elif endpoint['format'] == 'ollama':
                payload = {
                    "model": "llama3.2",
                    "prompt": prompt,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": temperature,
                        "top_p": 0.9,
                        "top_k": 40,
                        "repeat_penalty": 1.1
                    },
                    "stream": False
                }
            
            print(f"ğŸ“¤ Sending request to {endpoint['name']} with payload size: {len(str(payload))} bytes")
            
            # å‘é€è¯·æ±‚
            response = requests.post(endpoint['url'], json=payload, timeout=10)
            
            print(f"ğŸ“¥ Response from {endpoint['name']}: Status {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… Successfully connected to {endpoint['name']}")
                print(f"ğŸ“Š Response keys: {list(result.keys())}")
                
                # æ ¹æ®ä¸åŒæœåŠ¡è§£æå“åº”
                if endpoint['format'] == 'llamacpp':
                    content = result.get("content", "").strip()
                elif endpoint['format'] == 'textgen':
                    content = result.get("results", [{}])[0].get("text", "").strip()
                elif endpoint['format'] == 'openai':
                    content = result.get("choices", [{}])[0].get("text", "").strip()
                elif endpoint['format'] == 'ollama':
                    content = result.get("response", "").strip()
                else:
                    content = str(result)
                
                if content:
                    print(f"ğŸ¯ Generated content length: {len(content)} characters")
                    return content
                else:
                    print(f"âš ï¸ Empty response from {endpoint['name']}")
            else:
                print(f"âŒ HTTP Error {response.status_code} from {endpoint['name']}: {response.text[:200]}")
                
        except requests.exceptions.ConnectionError as e:
            print(f"ğŸ”Œ Connection failed to {endpoint['name']}: Service not running")
        except requests.exceptions.Timeout as e:
            print(f"â° Timeout connecting to {endpoint['name']}: {str(e)}")
        except requests.exceptions.RequestException as e:
            print(f"ğŸŒ Request error to {endpoint['name']}: {str(e)}")
        except Exception as e:
            print(f"ğŸ’¥ Unexpected error with {endpoint['name']}: {str(e)}")
    
    print(f"âŒ All LLM services unavailable. Checked {len(llm_endpoints)} endpoints.")
    return "LLM_SERVICE_UNAVAILABLE"

def simulate_ai_response(prompt: str, system_template: str = "default") -> str:
    """
    æ¨¡æ‹ŸAIå›å¤ï¼ˆå½“æ²¡æœ‰çœŸå®LLMæœåŠ¡æ—¶ä½¿ç”¨ï¼‰
    """
    
    # åŸºäºpromptå†…å®¹ç”Ÿæˆä¸åŒç±»å‹çš„å›å¤
    prompt_lower = prompt.lower()
    
    if "hello" in prompt_lower or "hi" in prompt_lower or "ä½ å¥½" in prompt_lower:
        return "Hello! I'm your AI assistant. How can I help you today?"
    
    elif "story" in prompt_lower or "creative" in prompt_lower:
        return """Once upon a time, in a world where technology and magic coexisted, there lived a young programmer who discovered that their code could bend reality itself. Each line they wrote didn't just execute on computers, but manifested in the physical world around them.

One day, while debugging a particularly stubborn algorithm, they accidentally created a small portal in their bedroom wall. Through it, they could see another dimension where their digital creations lived as sentient beings..."""
    
    elif "code" in prompt_lower or "programming" in prompt_lower:
        return """Here's a simple example of how you might approach this problem:

```python
def solve_problem(input_data):
    # Process the input
    result = []
    for item in input_data:
        # Apply your logic here
        processed_item = transform(item)
        result.append(processed_item)
    
    return result
```

Would you like me to explain any specific part or help you adapt this to your particular use case?"""
    
    elif "ä¸­æ–‡" in prompt or "chinese" in prompt_lower:
        return "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚æˆ‘å¯ä»¥ç”¨ä¸­æ–‡å’Œä½ å¯¹è¯ï¼Œå¸®åŠ©ä½ è§£å†³å„ç§é—®é¢˜ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
    
    else:
        return f"""I understand you're asking about: "{prompt}"

Based on your question, here are some key points to consider:

1. **Context**: This appears to be related to [relevant topic area]
2. **Approach**: I'd recommend starting with [suggested approach]
3. **Considerations**: Keep in mind [important factors]

Would you like me to elaborate on any of these points or help you with a more specific aspect of your question?"""

def handler(job):
    """
    RunPod serverless handler function with real AI
    
    Args:
        job (dict): Job data containing 'input' and 'id'
        
    Returns:
        str: AI generated response
    """
    try:
        job_input = job.get("input", {})
        job_id = job.get("id", "unknown")
        
        print(f"\nğŸš€ AI Job {job_id} started")
        log_gpu_status()
        
        # è·å–è¾“å…¥å‚æ•°
        prompt = job_input.get("prompt", "Hello!")
        system_template = job_input.get("system_template", "default")
        conversation_history = job_input.get("history", [])
        max_tokens = job_input.get("max_tokens", 1000)
        temperature = job_input.get("temperature", 0.7)
        
        print(f"ğŸ“ Processing prompt: '{prompt[:100]}{'...' if len(prompt) > 100 else ''}'")
        print(f"ğŸ­ Using system template: {system_template}")
        
        # è·å–ç³»ç»Ÿæç¤º
        system_prompt = SYSTEM_TEMPLATES.get(system_template, SYSTEM_TEMPLATES["default"])
        
        # æ ¼å¼åŒ–prompt
        formatted_prompt = format_llama3_prompt(system_prompt, prompt, conversation_history)
        
        print(f"ğŸ”§ Formatted prompt length: {len(formatted_prompt)} characters")
        
        # å°è¯•è°ƒç”¨çœŸå®LLMï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ¨¡æ‹Ÿå›å¤
        print(f"ğŸ¤– Attempting to load and use local LLM model...")
        print(f"ğŸ¯ Target model: Llama 3.2 8X3B MOE Dark Champion")
        print(f"ğŸ“ Expected model path: /models/ or similar")
        print(f"ğŸ”§ Formatted prompt preview: {formatted_prompt[:200]}...")
        
        try:
            ai_response = call_local_llm(formatted_prompt, max_tokens, temperature)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å¾—LLMå“åº”
            if ai_response == "LLM_SERVICE_UNAVAILABLE":
                print(f"âŒ No local LLM service found - all endpoints failed")
                print(f"ğŸ’¡ To use real AI, you need to:")
                print(f"   1. Install and run llama.cpp server on port 8080")
                print(f"   2. Or install text-generation-webui on port 5000")
                print(f"   3. Or install vLLM server on port 8000")
                print(f"   4. Or install Ollama on port 11434")
                print(f"ğŸ”„ Falling back to simulated AI response")
                ai_response = simulate_ai_response(prompt, system_template)
            elif "Error:" in ai_response or "Connection Error:" in ai_response:
                print(f"âš ï¸ LLM service error: {ai_response}")
                print(f"ğŸ”„ Falling back to simulated AI response")
                ai_response = simulate_ai_response(prompt, system_template)
            else:
                print(f"âœ… Successfully generated response using local LLM")
                print(f"ğŸ“ Response length: {len(ai_response)} characters")
                
        except Exception as e:
            print(f"ğŸ’¥ Unexpected error during LLM call: {str(e)}")
            print(f"ğŸ”„ Falling back to simulated AI response")
            ai_response = simulate_ai_response(prompt, system_template)
        
        print(f"âœ… Generated response: '{ai_response[:100]}{'...' if len(ai_response) > 100 else ''}'")
        log_gpu_status()
        print(f"ğŸ‰ AI Job {job_id} completed\n")
        
        return ai_response
        
    except Exception as e:
        error_msg = f"Handler error: {str(e)}"
        print(f"âŒ ERROR: {error_msg}")
        log_gpu_status()
        return f"I apologize, but I encountered an error while processing your request: {error_msg}"

def check_llm_services():
    """æ£€æŸ¥å¯ç”¨çš„LLMæœåŠ¡"""
    print("\nğŸ” Checking for available LLM services...")
    
    services = [
        ("llama.cpp server", "http://localhost:8080/health"),
        ("text-generation-webui", "http://localhost:5000/api/v1/model"),
        ("vLLM server", "http://localhost:8000/health"),
        ("Ollama", "http://localhost:11434/api/tags")
    ]
    
    available_services = []
    
    for service_name, health_url in services:
        try:
            response = requests.get(health_url, timeout=2)
            if response.status_code == 200:
                print(f"âœ… {service_name} is running and accessible")
                available_services.append(service_name)
            else:
                print(f"âš ï¸ {service_name} responded with status {response.status_code}")
        except requests.exceptions.ConnectionError:
            print(f"âŒ {service_name} is not running (connection refused)")
        except requests.exceptions.Timeout:
            print(f"â° {service_name} timeout (may be starting up)")
        except Exception as e:
            print(f"ğŸ’¥ {service_name} check failed: {str(e)}")
    
    if available_services:
        print(f"ğŸ‰ Found {len(available_services)} available LLM service(s): {', '.join(available_services)}")
    else:
        print("âš ï¸ No LLM services detected - will use simulated responses")
        print("ğŸ’¡ To enable real AI responses, install one of:")
        print("   â€¢ llama.cpp with server mode")
        print("   â€¢ text-generation-webui")
        print("   â€¢ vLLM")
        print("   â€¢ Ollama")
    
    return available_services

# å¯åŠ¨RunPod serverless
if __name__ == "__main__":
    print("=== RunPod AI Handler Starting ===")
    print(f"Python version: {sys.version}")
    print(f"RunPod version: {getattr(runpod, '__version__', 'unknown')}")
    
    # æ˜¾ç¤ºå¯ç”¨çš„ç³»ç»Ÿæ¨¡ç‰ˆ
    print("\nğŸ­ Available System Templates:")
    for template_name, template_desc in SYSTEM_TEMPLATES.items():
        print(f"  - {template_name}: {template_desc[:80]}{'...' if len(template_desc) > 80 else ''}")
    
    # æŸ¥æ‰¾å¹¶å°è¯•åŠ è½½æ¨¡å‹
    print("\n" + "="*50)
    model_files = find_model_files()
    
    if model_files:
        print(f"\nğŸ¯ Attempting to load the best model...")
        # å°è¯•åŠ è½½æœ€å¤§çš„æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯æœ€å¥½çš„ï¼‰
        best_model = model_files[0]
        
        if load_model(best_model):
            print(f"ğŸ‰ Successfully loaded model: {best_model['name']}")
            handler_mode = "Direct Model Loading"
        else:
            print(f"âŒ Failed to load model, checking LLM services...")
            # æ£€æŸ¥LLMæœåŠ¡ä½œä¸ºå¤‡é€‰
            available_llm_services = check_llm_services()
            handler_mode = f"LLM Services ({len(available_llm_services)} available)" if available_llm_services else "Simulated AI"
    else:
        print(f"âŒ No model files found, checking LLM services...")
        # æ£€æŸ¥LLMæœåŠ¡
        available_llm_services = check_llm_services()
        handler_mode = f"LLM Services ({len(available_llm_services)} available)" if available_llm_services else "Simulated AI"
    
    start_gpu_monitoring()
    
    print("\nğŸ” Initial GPU Status:")
    log_gpu_status()
    
    print(f"\nğŸš€ Starting RunPod AI serverless...")
    print(f"ğŸ”§ Handler Mode: {handler_mode}")
    print(f"ğŸ¯ Model Status: {'Loaded' if loaded_model else 'Not Loaded'}")
    print("="*50)
    
    runpod.serverless.start({"handler": handler}) 