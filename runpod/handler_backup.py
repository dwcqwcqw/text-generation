#!/usr/bin/env python3
"""
RunPod Handler for Llama GGUF Models
ä¿®å¤ "No module named runpod.serverless.start" é—®é¢˜
æ”¯æŒ L3.2-8X3B å’Œ L3.2-8X4B æ¨¡å‹
"""

import runpod
import json
import logging
import os
from llama_cpp import Llama

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹
loaded_models = {}
current_model_path = None

def load_model(model_path: str):
    """åŠ è½½Llamaæ¨¡å‹"""
    global loaded_models, current_model_path
    
    try:
        logger.info(f"å°è¯•åŠ è½½æ¨¡å‹: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(model_path) / (1024**3)  # GB
        logger.info(f"æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
        
        # å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œç›´æ¥è¿”å›
        if model_path in loaded_models:
            logger.info("æ¨¡å‹å·²åœ¨ç¼“å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨")
            current_model_path = model_path
            return loaded_models[model_path]
        
        # åŠ è½½æ¨¡å‹
        logger.info("å¼€å§‹åŠ è½½æ¨¡å‹...")
        model = Llama(
            model_path=model_path,
            n_ctx=2048,  # ä¸Šä¸‹æ–‡é•¿åº¦
            n_batch=512,  # æ‰¹å¤„ç†å¤§å°
            n_gpu_layers=-1,  # ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
            verbose=False
        )
        
        # ç¼“å­˜æ¨¡å‹
        loaded_models[model_path] = model
        current_model_path = model_path
        
        logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
        
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        raise e

def generate_text(model, prompt: str, max_tokens: int = 150, temperature: float = 0.7, 
                 top_p: float = 0.9, repeat_penalty: float = 1.05, stop_tokens: list = None):
    """ç”Ÿæˆæ–‡æœ¬"""
    try:
        logger.info(f"ç”Ÿæˆæ–‡æœ¬ - prompté•¿åº¦: {len(prompt)}, max_tokens: {max_tokens}")
        
        # é»˜è®¤åœæ­¢è¯
        if stop_tokens is None:
            stop_tokens = ["<|eot_id|>", "<|end_of_text|>", "<|start_header_id|>"]
        
        # ç”Ÿæˆæ–‡æœ¬
        output = model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            repeat_penalty=repeat_penalty,
            stop=stop_tokens,
            echo=False  # ä¸å›æ˜¾è¾“å…¥
        )
        
        generated_text = output['choices'][0]['text']
        logger.info(f"ç”Ÿæˆå®Œæˆ - è¾“å‡ºé•¿åº¦: {len(generated_text)}")
        
        return generated_text.strip()
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {e}")
        raise e

def handler(event):
    """RunPodå¤„ç†å‡½æ•°"""
    try:
        logger.info(f"æ”¶åˆ°è¯·æ±‚: {json.dumps(event, indent=2)}")
        
        # è§£æè¾“å…¥
        input_data = event.get("input", {})
        
        # è·å–å‚æ•°
        prompt = input_data.get("prompt", "")
        model_path = input_data.get("model_path", "/runpod-volume/text_models/L3.2-8X3B.gguf")
        max_tokens = input_data.get("max_tokens", 150)
        temperature = input_data.get("temperature", 0.7)
        top_p = input_data.get("top_p", 0.9)
        repeat_penalty = input_data.get("repeat_penalty", 1.05)
        stop_tokens = input_data.get("stop", ["<|eot_id|>", "<|end_of_text|>", "<|start_header_id|>"])
        stream = input_data.get("stream", False)
        
        # éªŒè¯è¾“å…¥
        if not prompt:
            return {
                "error": "æœªæä¾›prompt",
                "status": "FAILED"
            }
        
        # éªŒè¯æ¨¡å‹è·¯å¾„
        allowed_models = {
            "/runpod-volume/text_models/L3.2-8X3B.gguf": "Llama-3.2-8X3B (18.4B)",
            "/runpod-volume/text_models/L3.2-8X4B.gguf": "Llama-3.2-8X4B (21B)"
        }
        
        if model_path not in allowed_models:
            return {
                "error": f"ä¸æ”¯æŒçš„æ¨¡å‹è·¯å¾„: {model_path}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(allowed_models.keys())}",
                "status": "FAILED"
            }
        
        # åŠ è½½æ¨¡å‹
        try:
            model = load_model(model_path)
        except FileNotFoundError as e:
            return {
                "error": str(e),
                "status": "FAILED",
                "suggestion": "è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºæŒ‡å®šè·¯å¾„"
            }
        except Exception as e:
            return {
                "error": f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}",
                "status": "FAILED",
                "suggestion": "è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ ¼å¼å’Œç³»ç»Ÿå†…å­˜"
            }
        
        # ç”Ÿæˆæ–‡æœ¬
        try:
            generated_text = generate_text(
                model=model,
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                repeat_penalty=repeat_penalty,
                stop_tokens=stop_tokens
            )
            
            return {
                "text": generated_text,
                "status": "COMPLETED",
                "model_used": model_path,
                "model_name": allowed_models[model_path],
                "metadata": {
                    "prompt_length": len(prompt),
                    "response_length": len(generated_text),
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "repeat_penalty": repeat_penalty
                }
            }
            
        except Exception as e:
            return {
                "error": f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}",
                "status": "FAILED",
                "suggestion": "è¯·æ£€æŸ¥è¾“å…¥å‚æ•°å’Œæ¨¡å‹çŠ¶æ€"
            }
        
    except Exception as e:
        logger.error(f"Handleré”™è¯¯: {str(e)}")
        return {
            "error": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}",
            "status": "FAILED"
        }

def test_handler():
    """æµ‹è¯•handlerå‡½æ•°"""
    logger.info("å¼€å§‹æµ‹è¯•handler...")
    
    test_event = {
        "input": {
            "prompt": "Hello, how are you today?",
            "model_path": "/runpod-volume/text_models/L3.2-8X3B.gguf",
            "max_tokens": 50,
            "temperature": 0.7
        }
    }
    
    result = handler(test_event)
    print("æµ‹è¯•ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å¼
    test_handler()
else:
    # å¯åŠ¨RunPod serverless - å…¼å®¹å¤šç§ç‰ˆæœ¬
    logger.info("ğŸš€ å¯åŠ¨RunPod serverless handler...")
    try:
        # æ–°ç‰ˆæœ¬æ–¹å¼
        runpod.serverless.start({"handler": handler})
    except AttributeError:
        try:
            # å¤‡ç”¨æ–¹å¼1
            import runpod.serverless as serverless
            serverless.start(handler)
        except Exception:
            try:
                # å¤‡ç”¨æ–¹å¼2 - ç›´æ¥å¯åŠ¨
                runpod.start({"handler": handler})
            except Exception as e:
                logger.error(f"æ— æ³•å¯åŠ¨RunPod serverless: {e}")
                # æœ€ç®€å•çš„æ–¹å¼
                runpod.start(handler) 