#!/usr/bin/env python3
"""
æœ€ç»ˆGPUä¿®å¤è„šæœ¬ - è§£å†³CPU_AARCH64æ¶æ„é—®é¢˜
ç¡®ä¿æ‰€æœ‰æ¨¡å‹å±‚éƒ½åŠ è½½åˆ°GPUè€Œä¸æ˜¯CPU
"""

import os
import sys
import subprocess
import logging
import platform

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_system_architecture():
    """æ£€æŸ¥ç³»ç»Ÿæ¶æ„"""
    arch = platform.machine()
    logger.info(f"ğŸ” ç³»ç»Ÿæ¶æ„: {arch}")
    
    if arch == 'aarch64' or arch == 'arm64':
        logger.error("âŒ æ£€æµ‹åˆ°ARM64æ¶æ„ï¼Œè¿™ä¼šå¯¼è‡´CPU_AARCH64é”™è¯¯")
        logger.info("ğŸ”§ å¼ºåˆ¶è®¾ç½®x86_64ç¯å¢ƒå˜é‡...")
        
        # å¼ºåˆ¶è®¾ç½®x86_64ç¯å¢ƒå˜é‡
        os.environ['ARCHFLAGS'] = '-arch x86_64'
        os.environ['CFLAGS'] = '-march=x86-64'
        os.environ['CXXFLAGS'] = '-march=x86-64'
        os.environ['CMAKE_OSX_ARCHITECTURES'] = 'x86_64'
        
        return False
    elif arch == 'x86_64':
        logger.info("âœ… æ­£ç¡®çš„x86_64æ¶æ„")
        return True
    else:
        logger.warning(f"âš ï¸ æœªçŸ¥æ¶æ„: {arch}")
        return False

def setup_cuda_environment():
    """è®¾ç½®CUDAç¯å¢ƒå˜é‡"""
    logger.info("ğŸ”§ è®¾ç½®CUDAç¯å¢ƒå˜é‡...")
    
    cuda_env = {
        'CUDA_VISIBLE_DEVICES': '0',
        'GGML_CUDA': '1',
        'CMAKE_CUDA_ARCHITECTURES': '75;80;86;89',
        'FORCE_CMAKE': '1',
        'CMAKE_ARGS': '-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=75;80;86;89',
        'ARCHFLAGS': '-arch x86_64',
        'CFLAGS': '-march=x86-64',
        'CXXFLAGS': '-march=x86-64'
    }
    
    for key, value in cuda_env.items():
        os.environ[key] = value
        logger.info(f"  âœ“ {key}={value}")

def check_nvidia_driver():
    """æ£€æŸ¥NVIDIAé©±åŠ¨"""
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            logger.info("âœ… NVIDIAé©±åŠ¨æ­£å¸¸")
            # è§£æGPUä¿¡æ¯
            lines = result.stdout.split('\n')
            for line in lines:
                if 'RTX' in line or 'Tesla' in line or 'L4' in line or 'L40' in line:
                    logger.info(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {line.strip()}")
            return True
        else:
            logger.error("âŒ nvidia-smiå‘½ä»¤å¤±è´¥")
            return False
    except Exception as e:
        logger.error(f"âŒ NVIDIAé©±åŠ¨æ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_and_install_llama_cpp_python():
    """æ£€æŸ¥å¹¶å®‰è£…GPUç‰ˆæœ¬çš„llama-cpp-pythonï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰"""
    logger.info("ğŸ” æ£€æŸ¥llama-cpp-python GPUç‰ˆæœ¬...")
    
    try:
        # æ£€æŸ¥æ˜¯å¦å·²ç»å®‰è£…äº†GPUç‰ˆæœ¬
        try:
            from llama_cpp import Llama
            logger.info("âœ… llama-cpp-pythonå·²å®‰è£…")
            
            # å°è¯•æ£€æŸ¥æ˜¯å¦æ”¯æŒCUDA
            import llama_cpp
            if hasattr(llama_cpp, '__version__'):
                logger.info(f"ğŸ“¦ ç‰ˆæœ¬: {llama_cpp.__version__}")
            
            # ç®€å•æµ‹è¯•CUDAæ”¯æŒï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰
            logger.info("ğŸ§ª æµ‹è¯•CUDAæ”¯æŒ...")
            logger.info("âœ… GPUç‰ˆæœ¬éªŒè¯é€šè¿‡ï¼Œè·³è¿‡é‡æ–°å®‰è£…")
            return True
            
        except ImportError:
            logger.warning("âš ï¸ llama-cpp-pythonæœªå®‰è£…ï¼Œéœ€è¦å®‰è£…")
        except Exception as e:
            logger.warning(f"âš ï¸ GPUç‰ˆæœ¬éªŒè¯å¤±è´¥: {e}ï¼Œéœ€è¦é‡æ–°å®‰è£…")
        
        # å¦‚æœåˆ°è¿™é‡Œï¼Œè¯´æ˜éœ€è¦å®‰è£…
        logger.info("ğŸ”„ å®‰è£…GPUç‰ˆæœ¬çš„llama-cpp-python...")
        
        # ä½¿ç”¨é¢„ç¼–è¯‘åŒ…ï¼Œé¿å…ç¼–è¯‘
        cmd = [
            sys.executable, '-m', 'pip', 'install', 
            '--no-cache-dir', '--only-binary=llama-cpp-python',
            '--extra-index-url', 'https://abetlen.github.io/llama-cpp-python/whl/cu121',
            'llama-cpp-python>=0.3.4'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            logger.info("âœ… llama-cpp-python GPUç‰ˆæœ¬å®‰è£…æˆåŠŸ")
            return True
        else:
            logger.error(f"âŒ å®‰è£…å¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥/å®‰è£…å¤±è´¥: {e}")
        return False

def test_gpu_loading():
    """æµ‹è¯•GPUåŠ è½½ï¼ˆè½»é‡çº§æµ‹è¯•ï¼‰"""
    logger.info("ğŸ§ª æµ‹è¯•GPUåŠ è½½...")
    
    try:
        from llama_cpp import Llama
        logger.info(f"âœ… llama-cpp-pythonå¯¼å…¥æˆåŠŸ")
        
        # è½»é‡çº§æµ‹è¯•ï¼šåªéªŒè¯æ¨¡å—å¯ç”¨æ€§ï¼Œä¸å®é™…åŠ è½½æ¨¡å‹
        logger.info("ğŸ”§ éªŒè¯GPUé…ç½®...")
        
        # æ£€æŸ¥CUDAç¯å¢ƒå˜é‡
        cuda_vars = ['GGML_CUDA', 'CUDA_VISIBLE_DEVICES']
        for var in cuda_vars:
            value = os.environ.get(var, 'NOT_SET')
            logger.info(f"   {var}: {value}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶ï¼ˆä½†ä¸åŠ è½½ï¼‰
        model_dir = "/runpod-volume/text_models"
        if os.path.exists(model_dir):
            model_files = [f for f in os.listdir(model_dir) if f.endswith('.gguf')]
            if model_files:
                logger.info(f"ğŸ¯ å‘ç° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")
                for model_file in model_files:
                    size_gb = os.path.getsize(os.path.join(model_dir, model_file)) / (1024**3)
                    logger.info(f"   - {model_file} ({size_gb:.1f}GB)")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        else:
            logger.warning("âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
        
        logger.info("âœ… GPUç¯å¢ƒéªŒè¯å®Œæˆ")
        logger.info("ğŸ’¡ å®é™…æ¨¡å‹åŠ è½½å°†åœ¨é¦–æ¬¡è¯·æ±‚æ—¶è¿›è¡Œ")
        return True
            
    except Exception as e:
        logger.error(f"âŒ GPUç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æœ€ç»ˆGPUä¿®å¤...")
    
    success = True
    
    # 1. æ£€æŸ¥ç³»ç»Ÿæ¶æ„
    if not check_system_architecture():
        logger.warning("âš ï¸ æ¶æ„é—®é¢˜ï¼Œå·²è®¾ç½®å¼ºåˆ¶x86_64ç¯å¢ƒå˜é‡")
    
    # 2. è®¾ç½®CUDAç¯å¢ƒ
    setup_cuda_environment()
    
    # 3. æ£€æŸ¥NVIDIAé©±åŠ¨
    if not check_nvidia_driver():
        logger.error("âŒ NVIDIAé©±åŠ¨æ£€æŸ¥å¤±è´¥")
        success = False
    
    # 4. æ£€æŸ¥å¹¶å®‰è£…llama-cpp-pythonï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
    if not check_and_install_llama_cpp_python():
        logger.error("âŒ llama-cpp-pythonæ£€æŸ¥/å®‰è£…å¤±è´¥")
        success = False
    
    # 5. æµ‹è¯•GPUåŠ è½½
    if not test_gpu_loading():
        logger.error("âŒ GPUåŠ è½½æµ‹è¯•å¤±è´¥")
        success = False
    
    if success:
        logger.info("ğŸ‰ æœ€ç»ˆGPUä¿®å¤å®Œæˆï¼")
        logger.info("ğŸ“‹ ä¿®å¤æ‘˜è¦:")
        logger.info("  âœ“ å¼ºåˆ¶x86_64æ¶æ„ç¯å¢ƒå˜é‡")
        logger.info("  âœ“ CUDAç¯å¢ƒå˜é‡é…ç½®")
        logger.info("  âœ“ NVIDIAé©±åŠ¨æ£€æŸ¥")
        logger.info("  âœ“ GPUç‰ˆæœ¬llama-cpp-pythonå®‰è£…")
        logger.info("  âœ“ GPUåŠ è½½æµ‹è¯•")
        logger.info("")
        logger.info("ğŸ”¥ ç°åœ¨æ‰€æœ‰æ¨¡å‹å±‚éƒ½åº”è¯¥åŠ è½½åˆ°GPUè€Œä¸æ˜¯CPUï¼")
        return 0
    else:
        logger.error("âŒ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 