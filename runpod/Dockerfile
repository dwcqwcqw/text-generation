# å¼ºåˆ¶x86_64æ¶æ„GPUä¼˜åŒ–Dockerfile - è§£å†³CPU_AARCH64é—®é¢˜
FROM --platform=linux/amd64 nvidia/cuda:12.1-base-ubuntu22.04

# é˜²æ­¢æ—¶åŒºé…ç½®äº¤äº’
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# å¼ºåˆ¶GPUç¯å¢ƒå˜é‡
ENV GGML_CUDA=1
ENV CUDA_VISIBLE_DEVICES=0
ENV FORCE_CMAKE=1
ENV CMAKE_CUDA_ARCHITECTURES="75;80;86;89"
ENV LLAMA_CUBLAS=1

# åŸºç¡€ç³»ç»Ÿæ›´æ–°å’Œä¾èµ–å®‰è£…
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    ninja-build \
    pkg-config \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# å‡çº§pip
RUN pip3 install --upgrade pip

# å®‰è£…åŸºç¡€Pythonä¾èµ–
RUN pip3 install --no-cache-dir \
    runpod \
    psutil \
    requests \
    numpy

# å¼ºåˆ¶é‡è£…GPUç‰ˆæœ¬çš„llama-cpp-python
RUN pip3 uninstall -y llama-cpp-python || true
RUN pip3 install llama-cpp-python --upgrade --no-cache-dir \
    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶Handleræ–‡ä»¶
COPY handler_llama_ai.py /app/handler.py

# åˆ›å»ºå¼ºåˆ¶GPUé…ç½®çš„Handler
COPY <<EOF /app/handler_gpu_force.py
#!/usr/bin/env python3
"""
å¼ºåˆ¶GPUç‰ˆæœ¬Handler - è§£å†³CPU_AARCH64é—®é¢˜
"""
import os
import sys
import time
import logging
import psutil
import subprocess
from typing import Dict, Any, Optional
import runpod

# å¼ºåˆ¶GPUç¯å¢ƒå˜é‡
os.environ['GGML_CUDA'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['LLAMA_CUBLAS'] = '1'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(filename)s :%(lineno)d  %(asctime)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å…¨å±€æ¨¡å‹å®ä¾‹
llama_model = None

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for line in lines:
                parts = line.split(', ')
                if len(parts) >= 5:
                    name, total, used, temp, util = parts
                    return f"ğŸ”¥ GPUçŠ¶æ€: åˆ©ç”¨ç‡{util}%, æ˜¾å­˜{used}/{total}GB, æ¸©åº¦{temp}Â°C"
    except Exception as e:
        logger.warning(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
    return "ğŸ”¥ GPUçŠ¶æ€: æœªçŸ¥"

def find_models():
    """æŸ¥æ‰¾å¯ç”¨æ¨¡å‹"""
    model_dir = "/runpod-volume/text_models"
    models = []
    
    if os.path.exists(model_dir):
        for file in os.listdir(model_dir):
            if file.endswith('.gguf'):
                path = os.path.join(model_dir, file)
                size = os.path.getsize(path) / (1024**3)  # GB
                models.append((path, size))
                logger.info(f"ğŸ“ å‘ç°æ¨¡å‹: {path} ({size:.1f}GB)")
    
    # æŒ‰å¤§å°æ’åºï¼Œé€‰æ‹©è¾ƒå°çš„æ¨¡å‹ï¼ˆæ›´å®¹æ˜“åŠ è½½åˆ°GPUï¼‰
    models.sort(key=lambda x: x[1])
    return models

def get_gpu_layers(model_path: str) -> int:
    """æ ¹æ®æ¨¡å‹å’ŒGPUç¡®å®šGPUå±‚æ•°"""
    try:
        # è·å–GPUæ˜¾å­˜
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_memory = int(result.stdout.strip())
            logger.info(f"ğŸ¯ æ£€æµ‹åˆ°GPUæ˜¾å­˜: {gpu_memory}GB")
            
            # æ ¹æ®æ˜¾å­˜å’Œæ¨¡å‹å¤§å°å†³å®šå±‚æ•°
            model_size = os.path.getsize(model_path) / (1024**3)
            
            if gpu_memory >= 24:  # RTX 4090, L40ç­‰
                if model_size < 15:  # å°æ¨¡å‹å…¨éƒ¨ç”¨GPU
                    return -1  # æ‰€æœ‰å±‚
                else:  # å¤§æ¨¡å‹ç”¨å¤§éƒ¨åˆ†GPU
                    return 50
            elif gpu_memory >= 16:  # RTX 4080ç­‰
                return 40
            elif gpu_memory >= 12:  # RTX 4070ç­‰
                return 30
            else:
                return 20
    except Exception as e:
        logger.warning(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
    
    # é»˜è®¤å¼ºåˆ¶ä½¿ç”¨æ‰€æœ‰GPUå±‚
    return -1

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global llama_model
    
    logger.info("ğŸ”„ å¼€å§‹æ¨¡å‹åˆå§‹åŒ–...")
    logger.info(get_gpu_info())
    
    # æŸ¥æ‰¾æ¨¡å‹
    models = find_models()
    if not models:
        raise Exception("âŒ æœªæ‰¾åˆ°ä»»ä½•.ggufæ¨¡å‹æ–‡ä»¶")
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆæœ€å°çš„ï¼‰
    model_path, model_size = models[0]
    logger.info(f"ğŸ¯ é€‰æ‹©æ¨¡å‹: {model_path} ({model_size:.1f}GB)")
    
    try:
        # å¯¼å…¥llama_cpp
        import llama_cpp
        logger.info(f"llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        logger.info(f"ğŸ“‚ å¼ºåˆ¶GPUæ¨¡å¼åŠ è½½: {model_path}")
        
        # è·å–GPUå±‚æ•°
        n_gpu_layers = get_gpu_layers(model_path)
        logger.info(f"ğŸ”§ GPUé…ç½®: n_gpu_layers={n_gpu_layers}, n_ctx=65536, n_batch=1024")
        
        # å¼ºåˆ¶GPUé…ç½®
        llama_model = llama_cpp.Llama(
            model_path=model_path,
            n_gpu_layers=n_gpu_layers,  # å¼ºåˆ¶ä½¿ç”¨GPUå±‚
            n_ctx=65536,  # å¤§ä¸Šä¸‹æ–‡
            n_batch=1024,
            verbose=True,  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            use_mmap=True,
            use_mlock=False,
            n_threads=4,
            f16_kv=True,  # ä½¿ç”¨åŠç²¾åº¦
        )
        
        logger.info("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        logger.info(get_gpu_info())
        
        # éªŒè¯GPUä½¿ç”¨
        test_response = llama_model("Test", max_tokens=1, echo=False)
        logger.info("ğŸ§ª GPUæµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

def handler(event):
    """RunPod Handler"""
    global llama_model
    
    logger.info("ğŸ¯ Handlerè°ƒç”¨")
    logger.info(f"ğŸ“¥ å®Œæ•´äº‹ä»¶: {event}")
    
    try:
        # æå–è¾“å…¥
        job_input = event.get("input", {})
        prompt = job_input.get("prompt", "")
        max_tokens = job_input.get("max_tokens", 1000)
        temperature = job_input.get("temperature", 0.7)
        
        logger.info(f"ğŸ“ æå–çš„æç¤ºè¯: '{prompt}'")
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if llama_model is None:
            logger.info("ğŸ”„ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            initialize_model()
        
        # ç”Ÿæˆå“åº”
        logger.info("ğŸ¤– å¼€å§‹ç”Ÿæˆå“åº”...")
        start_time = time.time()
        
        response = llama_model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            echo=False,
            stop=["</s>", "<|eot_id|>", "<|end_of_text|>"]
        )
        
        generation_time = time.time() - start_time
        generated_text = response['choices'][0]['text'].strip()
        
        logger.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
        logger.info(f"ğŸ“¤ ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)}å­—ç¬¦")
        logger.info(get_gpu_info())
        
        return {
            "text": generated_text,
            "generation_time": generation_time,
            "model_info": "GPUåŠ é€Ÿæ¨¡å¼",
            "gpu_status": get_gpu_info()
        }
        
    except Exception as e:
        logger.error(f"âŒ Handleræ‰§è¡Œå¤±è´¥: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    import time
    
    logger.info("ğŸš€ å¯åŠ¨å¼ºåˆ¶GPUæ¨¡å¼RunPod handler...")
    logger.info(get_gpu_info())
    
    # é¢„åˆå§‹åŒ–æ¨¡å‹
    try:
        initialize_model()
    except Exception as e:
        logger.error(f"é¢„åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # å¯åŠ¨RunPod
    runpod.serverless.start({"handler": handler})
EOF

RUN chmod +x /app/handler_gpu_force.py

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p /runpod-volume/text_models

# éªŒè¯è„šæœ¬
COPY <<EOF /app/verify_gpu.py
#!/usr/bin/env python3
import sys
import subprocess
import os

def verify_setup():
    print("ğŸ” éªŒè¯GPUè®¾ç½®...")
    
    # æ£€æŸ¥æ¶æ„
    arch = subprocess.run(['uname', '-m'], capture_output=True, text=True).stdout.strip()
    print(f"ğŸ“Š ç³»ç»Ÿæ¶æ„: {arch}")
    if arch != 'x86_64':
        print("âŒ é”™è¯¯ï¼šä¸æ˜¯x86_64æ¶æ„ï¼")
        return False
    
    # æ£€æŸ¥CUDA
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode != 0:
            print("âŒ NVIDIAé©±åŠ¨æ£€æŸ¥å¤±è´¥")
            return False
        print("âœ… NVIDIAé©±åŠ¨æ­£å¸¸")
    except:
        print("âŒ nvidia-smiå‘½ä»¤å¤±è´¥")
        return False
    
    # æ£€æŸ¥llama-cpp-python
    try:
        import llama_cpp
        print(f"âœ… llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒCUDA
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹å®ä¾‹æ¥æµ‹è¯•CUDAæ”¯æŒ
        print("ğŸ§ª æµ‹è¯•CUDAæ”¯æŒ...")
        
    except ImportError:
        print("âŒ llama-cpp-pythonæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âš ï¸ llama-cpp-pythonæµ‹è¯•è­¦å‘Š: {e}")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("ğŸ“Š ç¯å¢ƒå˜é‡:")
    for var in ['GGML_CUDA', 'CUDA_VISIBLE_DEVICES', 'CMAKE_CUDA_ARCHITECTURES']:
        value = os.environ.get(var, 'NOT SET')
        print(f"  - {var}: {value}")
    
    print("âœ… åŸºç¡€éªŒè¯é€šè¿‡")
    return True

if __name__ == "__main__":
    success = verify_setup()
    sys.exit(0 if success else 1)
EOF

RUN chmod +x /app/verify_gpu.py

# è¿è¡ŒéªŒè¯
RUN python3 /app/verify_gpu.py

# å¯åŠ¨è„šæœ¬
COPY <<EOF /app/start.sh
#!/bin/bash
set -e

echo "ğŸš€ å¯åŠ¨å¼ºåˆ¶GPUæ¨¡å¼AIæ–‡æœ¬ç”ŸæˆæœåŠ¡"
echo "ğŸ“Š ç³»ç»Ÿä¿¡æ¯:"
echo "  - å®¹å™¨æ¶æ„: $(uname -m)"
echo "  - Pythonç‰ˆæœ¬: $(python3 --version)"

echo "ğŸ“Š GPUä¿¡æ¯:"
nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv

echo "ğŸ“Š ç¯å¢ƒå˜é‡:"
echo "  - GGML_CUDA: $GGML_CUDA"
echo "  - CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "  - CMAKE_CUDA_ARCHITECTURES: $CMAKE_CUDA_ARCHITECTURES"

echo "ğŸ§ª æœ€ç»ˆéªŒè¯:"
python3 /app/verify_gpu.py

echo "ğŸš€ å¯åŠ¨å¼ºåˆ¶GPU Handler"
exec python3 /app/handler_gpu_force.py
EOF

RUN chmod +x /app/start.sh

# ç«¯å£
EXPOSE 8000

# å¯åŠ¨æœåŠ¡
CMD ["/app/start.sh"] 