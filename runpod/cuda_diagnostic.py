#!/usr/bin/env python3
"""
CUDAè¯Šæ–­è„šæœ¬ - æ£€æŸ¥llama-cpp-pythonçš„CUDAæ”¯æŒ
"""

import os
import sys
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡"""
    logger.info("=== ç¯å¢ƒå˜é‡æ£€æŸ¥ ===")
    cuda_vars = [
        'CUDA_VISIBLE_DEVICES',
        'LLAMA_CUBLAS', 
        'CMAKE_ARGS',
        'FORCE_CMAKE',
        'CUDA_LAUNCH_BLOCKING'
    ]
    
    for var in cuda_vars:
        value = os.environ.get(var, 'Not set')
        logger.info(f"{var}: {value}")

def check_nvidia_smi():
    """æ£€æŸ¥nvidia-smi"""
    logger.info("=== nvidia-smiæ£€æŸ¥ ===")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            logger.info("nvidia-smiè¾“å‡º:")
            print(result.stdout)
        else:
            logger.error(f"nvidia-smiå¤±è´¥: {result.stderr}")
    except Exception as e:
        logger.error(f"nvidia-smié”™è¯¯: {e}")

def check_pytorch_cuda():
    """æ£€æŸ¥PyTorch CUDAæ”¯æŒ"""
    logger.info("=== PyTorch CUDAæ£€æŸ¥ ===")
    try:
        import torch
        logger.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        logger.info(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            logger.info(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                props = torch.cuda.get_device_properties(i)
                logger.info(f"  æ˜¾å­˜: {props.total_memory / 1024**3:.1f}GB")
                logger.info(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    except Exception as e:
        logger.error(f"PyTorchæ£€æŸ¥å¤±è´¥: {e}")

def check_llama_cpp_cuda():
    """æ£€æŸ¥llama-cpp-pythonçš„CUDAæ”¯æŒ"""
    logger.info("=== llama-cpp-python CUDAæ£€æŸ¥ ===")
    try:
        import llama_cpp
        logger.info(f"llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰CUDAæ”¯æŒ
        try:
            # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹å®ä¾‹æ¥æµ‹è¯•CUDA
            logger.info("æµ‹è¯•CUDAæ”¯æŒ...")
            
            # æ£€æŸ¥llama_cppçš„å†…éƒ¨CUDAæ”¯æŒ
            if hasattr(llama_cpp, '_lib'):
                logger.info("llama_cpp._libå­˜åœ¨")
            else:
                logger.warning("llama_cpp._libä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"CUDAæ”¯æŒæµ‹è¯•å¤±è´¥: {e}")
            
    except ImportError as e:
        logger.error(f"æ— æ³•å¯¼å…¥llama-cpp-python: {e}")

def check_cuda_libraries():
    """æ£€æŸ¥CUDAåº“"""
    logger.info("=== CUDAåº“æ£€æŸ¥ ===")
    try:
        result = subprocess.run(['ldconfig', '-p'], capture_output=True, text=True)
        if result.returncode == 0:
            cuda_libs = [line for line in result.stdout.split('\n') if 'cuda' in line.lower()]
            if cuda_libs:
                logger.info("æ‰¾åˆ°CUDAåº“:")
                for lib in cuda_libs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    logger.info(f"  {lib.strip()}")
            else:
                logger.warning("æœªæ‰¾åˆ°CUDAåº“")
    except Exception as e:
        logger.error(f"CUDAåº“æ£€æŸ¥å¤±è´¥: {e}")

def test_simple_model():
    """æµ‹è¯•ç®€å•æ¨¡å‹åŠ è½½"""
    logger.info("=== ç®€å•æ¨¡å‹æµ‹è¯• ===")
    try:
        from llama_cpp import Llama
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        model_paths = [
            "/runpod-volume/text_models/L3.2-8X4B.gguf",
            "/runpod-volume/text_models/L3.2-8X3B.gguf"
        ]
        
        model_path = None
        for path in model_paths:
            if os.path.exists(path):
                model_path = path
                break
                
        if not model_path:
            logger.error("æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            return
            
        logger.info(f"æµ‹è¯•åŠ è½½æ¨¡å‹: {model_path}")
        
        # æµ‹è¯•GPUæ¨¡å¼
        logger.info("æµ‹è¯•GPUæ¨¡å¼...")
        try:
            model = Llama(
                model_path=model_path,
                n_ctx=512,
                n_batch=128,
                n_gpu_layers=1,  # åªæµ‹è¯•1å±‚
                verbose=True
            )
            logger.info("âœ… GPUæ¨¡å¼æµ‹è¯•æˆåŠŸ")
            del model
        except Exception as e:
            logger.error(f"âŒ GPUæ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
            
    except Exception as e:
        logger.error(f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    logger.info("ğŸ” å¼€å§‹CUDAè¯Šæ–­...")
    
    check_environment()
    check_nvidia_smi()
    check_pytorch_cuda()
    check_llama_cpp_cuda()
    check_cuda_libraries()
    test_simple_model()
    
    logger.info("ğŸ” CUDAè¯Šæ–­å®Œæˆ") 