#!/usr/bin/env python3
"""
GPUå®‰è£…éªŒè¯è„šæœ¬ - åœ¨æ„å»ºæ—¶éªŒè¯llama-cpp-pythonçš„GPUæ”¯æŒ
"""

import sys
import os

def verify_gpu_support():
    """éªŒè¯GPUæ”¯æŒ"""
    print("ğŸ” éªŒè¯llama-cpp-python GPUæ”¯æŒ...")
    
    try:
        # æ£€æŸ¥CUDAç¯å¢ƒå˜é‡
        cuda_vars = ['GGML_CUDA', 'CUDA_VISIBLE_DEVICES']
        for var in cuda_vars:
            value = os.environ.get(var, 'NOT_SET')
            print(f"   {var}: {value}")
        
        # å°è¯•å¯¼å…¥llama-cpp-python
        from llama_cpp import Llama
        print("âœ… llama-cpp-pythonå¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥ç‰ˆæœ¬
        version = getattr(Llama, '__version__', 'unknown')
        print(f"   ç‰ˆæœ¬: {version}")
        
        # å°è¯•æ£€æŸ¥CUDAæ”¯æŒ
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„ç¼–è¯‘çš„GPUç‰ˆæœ¬
            print("ğŸ§ª æµ‹è¯•CUDAæ”¯æŒ...")
            
            # æ£€æŸ¥æ¨¡å—å±æ€§
            import llama_cpp
            if hasattr(llama_cpp, 'llama_cpp'):
                print("âœ… æ£€æµ‹åˆ°C++æ‰©å±•æ¨¡å—")
            
            # éªŒè¯ç¯å¢ƒå˜é‡
            cuda_env_ok = os.environ.get('GGML_CUDA') == '1'
            if cuda_env_ok:
                print("âœ… CUDAç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®")
            else:
                print("âš ï¸ CUDAç¯å¢ƒå˜é‡æœªè®¾ç½®")
            
            print("âœ… GPUæ”¯æŒéªŒè¯é€šè¿‡")
            return True
            
        except Exception as cuda_error:
            print(f"âŒ CUDAæ”¯æŒæµ‹è¯•å¤±è´¥: {cuda_error}")
            return False
            
    except ImportError as e:
        print(f"âŒ llama-cpp-pythonå¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹GPUå®‰è£…éªŒè¯...")
    
    success = verify_gpu_support()
    
    if success:
        print("âœ… GPUå®‰è£…éªŒè¯æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("âŒ GPUå®‰è£…éªŒè¯å¤±è´¥ï¼")
        sys.exit(1)

if __name__ == "__main__":
    main() 