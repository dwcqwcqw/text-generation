# RTX 4090ä¸“ç”¨Dockerfile - é’ˆå¯¹Ada Lovelaceæ¶æ„(è®¡ç®—èƒ½åŠ›8.9)ä¼˜åŒ–
FROM nvidia/cuda:12.3-devel-ubuntu22.04

# é˜²æ­¢æ—¶åŒºé…ç½®äº¤äº’
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# RTX 4090ä¸“ç”¨ç¯å¢ƒå˜é‡
ENV GGML_CUDA=1
ENV CUDA_VISIBLE_DEVICES=0
ENV CMAKE_CUDA_ARCHITECTURES=89
ENV NVCC_APPEND_FLAGS="--allow-unsupported-compiler"
ENV CUDA_LAUNCH_BLOCKING=1

# åŸºç¡€ç³»ç»Ÿæ›´æ–°å’Œä¾èµ–å®‰è£…
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    ninja-build \
    pkg-config \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
RUN pip3 install --no-cache-dir \
    runpod \
    psutil \
    requests \
    numpy

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶RTX 4090ä¿®å¤è„šæœ¬
COPY fix_cuda_rtx4090.py /app/
COPY handler_rtx4090.py /app/handler.py

# è®¾ç½®RTX 4090ä¸“ç”¨æƒé™
RUN chmod +x /app/fix_cuda_rtx4090.py

# è¿è¡ŒRTX 4090 CUDAä¿®å¤
RUN python3 /app/fix_cuda_rtx4090.py

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p /runpod-volume/text_models

# RTX 4090å¥åº·æ£€æŸ¥
COPY <<EOF /app/rtx4090_health_check.py
#!/usr/bin/env python3
import sys
import subprocess

def check_rtx4090():
    try:
        # æ£€æŸ¥CUDA
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode != 0:
            print("âŒ NVIDIAé©±åŠ¨æ£€æŸ¥å¤±è´¥")
            return False
            
        # æ£€æŸ¥GPUå‹å·
        gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], 
                                  capture_output=True, text=True)
        if gpu_result.returncode == 0:
            gpu_name = gpu_result.stdout.strip()
            print(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {gpu_name}")
            
            if "4090" in gpu_name:
                print("âœ… ç¡®è®¤RTX 4090")
            else:
                print("âš ï¸ éRTX 4090ï¼Œä½†å¯ä»¥è¿è¡Œ")
        
        # æ£€æŸ¥llama-cpp-python
        import llama_cpp
        print(f"âœ… llama-cpp-python: {llama_cpp.__version__}")
        
        return True
        
    except Exception as e:
        print(f"âŒ RTX 4090æ£€æŸ¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = check_rtx4090()
    sys.exit(0 if success else 1)
EOF

RUN chmod +x /app/rtx4090_health_check.py

# è¿è¡ŒRTX 4090å¥åº·æ£€æŸ¥
RUN python3 /app/rtx4090_health_check.py

# è®¾ç½®RTX 4090å¯åŠ¨è„šæœ¬
COPY <<EOF /app/start_rtx4090.sh
#!/bin/bash
set -e

echo "ğŸš€ å¯åŠ¨RTX 4090ä¸“ç”¨æœåŠ¡"
echo "ğŸ“Š GPUä¿¡æ¯:"
nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv

echo "ğŸ“Š ç¯å¢ƒå˜é‡:"
echo "GGML_CUDA: $GGML_CUDA"
echo "CMAKE_CUDA_ARCHITECTURES: $CMAKE_CUDA_ARCHITECTURES"

echo "ğŸ§ª éªŒè¯llama-cpp-python CUDAæ”¯æŒ:"
python3 -c "
import llama_cpp
print('âœ… llama-cpp-pythonå¯ç”¨')
print(f'ç‰ˆæœ¬: {llama_cpp.__version__}')
"

echo "ğŸš€ å¯åŠ¨RTX 4090 Handler"
exec python3 /app/handler.py
EOF

RUN chmod +x /app/start_rtx4090.sh

# RTX 4090ä¸“ç”¨ç«¯å£
EXPOSE 8000

# å¯åŠ¨RTX 4090æœåŠ¡
CMD ["/app/start_rtx4090.sh"] 