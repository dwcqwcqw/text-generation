# RTX 4090ä¸“ç”¨ä¼˜åŒ–Dockerfile - å¼ºåˆ¶x86_64æ¶æ„å’ŒGPUåŠ é€Ÿ
FROM --platform=linux/amd64 nvidia/cuda:12.1-base-ubuntu22.04

# é˜²æ­¢æ—¶åŒºé…ç½®äº¤äº’
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# RTX 4090ä¸“ç”¨GPUç¯å¢ƒå˜é‡
ENV GGML_CUDA=1
ENV CUDA_VISIBLE_DEVICES=0
ENV FORCE_CMAKE=1
ENV CMAKE_CUDA_ARCHITECTURES="89"
ENV LLAMA_CUBLAS=1
ENV CUDA_LAUNCH_BLOCKING=1

# åŸºç¡€ç³»ç»Ÿæ›´æ–°å’Œä¾èµ–å®‰è£…
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    ninja-build \
    pkg-config \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# å‡çº§pip
RUN pip3 install --upgrade pip

# å®‰è£…åŸºç¡€Pythonä¾èµ–
RUN pip3 install --no-cache-dir \
    runpod \
    psutil \
    requests \
    numpy

# å¼ºåˆ¶é‡è£…RTX 4090ä¼˜åŒ–çš„GPUç‰ˆæœ¬llama-cpp-python
RUN pip3 uninstall -y llama-cpp-python || true
RUN CMAKE_CUDA_ARCHITECTURES=89 pip3 install llama-cpp-python --upgrade --no-cache-dir \
    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# åˆ›å»ºRTX 4090ä¸“ç”¨Handler
COPY <<EOF /app/handler_rtx4090.py
#!/usr/bin/env python3
"""
RTX 4090ä¸“ç”¨Handler - 24GBæ˜¾å­˜ä¼˜åŒ–
"""
import os
import sys
import time
import logging
import psutil
import subprocess
from typing import Dict, Any, Optional
import runpod

# RTX 4090ä¸“ç”¨ç¯å¢ƒå˜é‡
os.environ['GGML_CUDA'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['LLAMA_CUBLAS'] = '1'
os.environ['CMAKE_CUDA_ARCHITECTURES'] = '89'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(filename)s :%(lineno)d  %(asctime)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å…¨å±€æ¨¡å‹å®ä¾‹
llama_model = None

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for line in lines:
                parts = line.split(', ')
                if len(parts) >= 5:
                    name, total, used, temp, util = parts
                    return f"ğŸ”¥ RTX 4090çŠ¶æ€: åˆ©ç”¨ç‡{util}%, æ˜¾å­˜{used}/{total}GB, æ¸©åº¦{temp}Â°C"
    except Exception as e:
        logger.warning(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
    return "ğŸ”¥ GPUçŠ¶æ€: æœªçŸ¥"

def find_models():
    """æŸ¥æ‰¾å¯ç”¨æ¨¡å‹"""
    model_dir = "/runpod-volume/text_models"
    models = []
    
    if os.path.exists(model_dir):
        for file in os.listdir(model_dir):
            if file.endswith('.gguf'):
                path = os.path.join(model_dir, file)
                size = os.path.getsize(path) / (1024**3)  # GB
                models.append((path, size))
                logger.info(f"ğŸ“ å‘ç°æ¨¡å‹: {path} ({size:.1f}GB)")
    
    # æŒ‰å¤§å°æ’åºï¼ŒRTX 4090ä¼˜å…ˆé€‰æ‹©è¾ƒå¤§çš„æ¨¡å‹
    models.sort(key=lambda x: x[1], reverse=True)
    return models

def get_rtx4090_config(model_path: str) -> dict:
    """RTX 4090ä¸“ç”¨é…ç½®"""
    model_size = os.path.getsize(model_path) / (1024**3)
    
    if model_size > 18:  # L3.2-8X3B (18.2GB)
        return {
            'n_gpu_layers': 45,  # å¤§æ¨¡å‹ç”¨45å±‚GPU
            'n_ctx': 16384,      # 16Kä¸Šä¸‹æ–‡
            'n_batch': 512,
        }
    elif model_size > 13:  # L3.2-8X4B (13.9GB)
        return {
            'n_gpu_layers': 60,  # ä¸­ç­‰æ¨¡å‹ç”¨60å±‚GPU
            'n_ctx': 32768,      # 32Kä¸Šä¸‹æ–‡
            'n_batch': 1024,
        }
    else:  # å°æ¨¡å‹
        return {
            'n_gpu_layers': -1,  # æ‰€æœ‰å±‚ç”¨GPU
            'n_ctx': 65536,      # 64Kä¸Šä¸‹æ–‡
            'n_batch': 1024,
        }

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global llama_model
    
    logger.info("ğŸ”„ RTX 4090æ¨¡å‹åˆå§‹åŒ–...")
    logger.info(get_gpu_info())
    
    # æŸ¥æ‰¾æ¨¡å‹
    models = find_models()
    if not models:
        raise Exception("âŒ æœªæ‰¾åˆ°ä»»ä½•.ggufæ¨¡å‹æ–‡ä»¶")
    
    # RTX 4090ä¼˜å…ˆé€‰æ‹©æœ€å¤§çš„æ¨¡å‹
    model_path, model_size = models[0]
    logger.info(f"ğŸ¯ RTX 4090é€‰æ‹©æ¨¡å‹: {model_path} ({model_size:.1f}GB)")
    
    try:
        # å¯¼å…¥llama_cpp
        import llama_cpp
        logger.info(f"llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        logger.info(f"ğŸ“‚ RTX 4090å¼ºåˆ¶GPUæ¨¡å¼åŠ è½½: {model_path}")
        
        # è·å–RTX 4090ä¸“ç”¨é…ç½®
        config = get_rtx4090_config(model_path)
        logger.info(f"ğŸ”§ RTX 4090é…ç½®: {config}")
        
        # RTX 4090ä¼˜åŒ–é…ç½®
        llama_model = llama_cpp.Llama(
            model_path=model_path,
            n_gpu_layers=config['n_gpu_layers'],
            n_ctx=config['n_ctx'],
            n_batch=config['n_batch'],
            verbose=True,  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            use_mmap=True,
            use_mlock=False,
            n_threads=8,  # RTX 4090å¯ä»¥ç”¨æ›´å¤šçº¿ç¨‹
            f16_kv=True,  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
            offload_kqv=True,  # å¸è½½KVç¼“å­˜åˆ°GPU
        )
        
        logger.info("âœ… RTX 4090æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        logger.info(get_gpu_info())
        
        # éªŒè¯GPUä½¿ç”¨
        test_response = llama_model("Test", max_tokens=1, echo=False)
        logger.info("ğŸ§ª RTX 4090 GPUæµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ RTX 4090æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

def handler(event):
    """RTX 4090ä¼˜åŒ–Handler"""
    global llama_model
    
    logger.info("ğŸ¯ RTX 4090 Handlerè°ƒç”¨")
    logger.info(f"ğŸ“¥ å®Œæ•´äº‹ä»¶: {event}")
    
    try:
        # æå–è¾“å…¥
        job_input = event.get("input", {})
        prompt = job_input.get("prompt", "")
        max_tokens = job_input.get("max_tokens", 1000)
        temperature = job_input.get("temperature", 0.7)
        
        logger.info(f"ğŸ“ æå–çš„æç¤ºè¯: '{prompt}'")
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if llama_model is None:
            logger.info("ğŸ”„ RTX 4090æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            initialize_model()
        
        # ç”Ÿæˆå“åº”
        logger.info("ğŸ¤– RTX 4090å¼€å§‹ç”Ÿæˆå“åº”...")
        start_time = time.time()
        
        response = llama_model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            echo=False,
            stop=["</s>", "<|eot_id|>", "<|end_of_text|>"]
        )
        
        generation_time = time.time() - start_time
        generated_text = response['choices'][0]['text'].strip()
        
        logger.info(f"âœ… RTX 4090ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
        logger.info(f"ğŸ“¤ ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)}å­—ç¬¦")
        logger.info(get_gpu_info())
        
        return {
            "text": generated_text,
            "generation_time": generation_time,
            "model_info": "RTX 4090 GPUåŠ é€Ÿæ¨¡å¼",
            "gpu_status": get_gpu_info()
        }
        
    except Exception as e:
        logger.error(f"âŒ RTX 4090 Handleræ‰§è¡Œå¤±è´¥: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨RTX 4090ä¸“ç”¨RunPod handler...")
    logger.info(get_gpu_info())
    
    # é¢„åˆå§‹åŒ–æ¨¡å‹
    try:
        initialize_model()
    except Exception as e:
        logger.error(f"RTX 4090é¢„åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # å¯åŠ¨RunPod
    runpod.serverless.start({"handler": handler})
EOF

RUN chmod +x /app/handler_rtx4090.py

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p /runpod-volume/text_models

# RTX 4090éªŒè¯è„šæœ¬
COPY <<EOF /app/verify_rtx4090.py
#!/usr/bin/env python3
import sys
import subprocess
import os

def verify_rtx4090():
    print("ğŸ” éªŒè¯RTX 4090è®¾ç½®...")
    
    # æ£€æŸ¥æ¶æ„
    arch = subprocess.run(['uname', '-m'], capture_output=True, text=True).stdout.strip()
    print(f"ğŸ“Š ç³»ç»Ÿæ¶æ„: {arch}")
    if arch != 'x86_64':
        print("âŒ é”™è¯¯ï¼šä¸æ˜¯x86_64æ¶æ„ï¼")
        return False
    
    # æ£€æŸ¥GPUå‹å·
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_name = result.stdout.strip()
            print(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {gpu_name}")
            if 'RTX 4090' not in gpu_name:
                print(f"âš ï¸ è­¦å‘Šï¼šä¸æ˜¯RTX 4090ï¼Œè€Œæ˜¯ {gpu_name}")
        else:
            print("âŒ æ— æ³•æ£€æµ‹GPUå‹å·")
            return False
    except:
        print("âŒ nvidia-smiå‘½ä»¤å¤±è´¥")
        return False
    
    # æ£€æŸ¥è®¡ç®—èƒ½åŠ›
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=compute_cap', '--format=csv,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            compute_cap = result.stdout.strip()
            print(f"ğŸ”§ è®¡ç®—èƒ½åŠ›: {compute_cap}")
            if compute_cap != '8.9':
                print(f"âš ï¸ è­¦å‘Šï¼šè®¡ç®—èƒ½åŠ›ä¸æ˜¯8.9ï¼Œè€Œæ˜¯ {compute_cap}")
    except:
        print("âš ï¸ æ— æ³•æ£€æµ‹è®¡ç®—èƒ½åŠ›")
    
    # æ£€æŸ¥llama-cpp-python
    try:
        import llama_cpp
        print(f"âœ… llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
    except ImportError:
        print("âŒ llama-cpp-pythonæœªå®‰è£…")
        return False
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("ğŸ“Š RTX 4090ç¯å¢ƒå˜é‡:")
    expected_vars = {
        'GGML_CUDA': '1',
        'CUDA_VISIBLE_DEVICES': '0',
        'CMAKE_CUDA_ARCHITECTURES': '89'
    }
    
    for var, expected in expected_vars.items():
        value = os.environ.get(var, 'NOT SET')
        status = "âœ…" if value == expected else "âŒ"
        print(f"  {status} {var}: {value} (æœŸæœ›: {expected})")
    
    print("âœ… RTX 4090éªŒè¯å®Œæˆ")
    return True

if __name__ == "__main__":
    success = verify_rtx4090()
    sys.exit(0 if success else 1)
EOF

RUN chmod +x /app/verify_rtx4090.py

# è¿è¡ŒRTX 4090éªŒè¯
RUN python3 /app/verify_rtx4090.py

# RTX 4090å¯åŠ¨è„šæœ¬
COPY <<EOF /app/start_rtx4090.sh
#!/bin/bash
set -e

echo "ğŸš€ å¯åŠ¨RTX 4090ä¸“ç”¨AIæ–‡æœ¬ç”ŸæˆæœåŠ¡"
echo "ğŸ“Š ç³»ç»Ÿä¿¡æ¯:"
echo "  - å®¹å™¨æ¶æ„: $(uname -m)"
echo "  - Pythonç‰ˆæœ¬: $(python3 --version)"

echo "ğŸ“Š RTX 4090 GPUä¿¡æ¯:"
nvidia-smi --query-gpu=name,memory.total,compute_cap,driver_version --format=csv

echo "ğŸ“Š RTX 4090ç¯å¢ƒå˜é‡:"
echo "  - GGML_CUDA: $GGML_CUDA"
echo "  - CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "  - CMAKE_CUDA_ARCHITECTURES: $CMAKE_CUDA_ARCHITECTURES"

echo "ğŸ§ª RTX 4090æœ€ç»ˆéªŒè¯:"
python3 /app/verify_rtx4090.py

echo "ğŸš€ å¯åŠ¨RTX 4090ä¸“ç”¨Handler"
exec python3 /app/handler_rtx4090.py
EOF

RUN chmod +x /app/start_rtx4090.sh

# ç«¯å£
EXPOSE 8000

# å¯åŠ¨RTX 4090æœåŠ¡
CMD ["/app/start_rtx4090.sh"] 