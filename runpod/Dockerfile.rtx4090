# RTX 4090专用优化Dockerfile - 强制x86_64架构和GPU加速
FROM --platform=linux/amd64 nvidia/cuda:12.1-base-ubuntu22.04

# 防止时区配置交互
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC

# RTX 4090专用GPU环境变量
ENV GGML_CUDA=1
ENV CUDA_VISIBLE_DEVICES=0
ENV FORCE_CMAKE=1
ENV CMAKE_CUDA_ARCHITECTURES="89"
ENV LLAMA_CUBLAS=1
ENV CUDA_LAUNCH_BLOCKING=1

# 基础系统更新和依赖安装
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    ninja-build \
    pkg-config \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# 升级pip
RUN pip3 install --upgrade pip

# 安装基础Python依赖
RUN pip3 install --no-cache-dir \
    runpod \
    psutil \
    requests \
    numpy

# 强制重装RTX 4090优化的GPU版本llama-cpp-python
RUN pip3 uninstall -y llama-cpp-python || true
RUN CMAKE_CUDA_ARCHITECTURES=89 pip3 install llama-cpp-python --upgrade --no-cache-dir \
    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# 设置工作目录
WORKDIR /app

# 创建RTX 4090专用Handler
COPY <<EOF /app/handler_rtx4090.py
#!/usr/bin/env python3
"""
RTX 4090专用Handler - 24GB显存优化
"""
import os
import sys
import time
import logging
import psutil
import subprocess
from typing import Dict, Any, Optional
import runpod

# RTX 4090专用环境变量
os.environ['GGML_CUDA'] = '1'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['LLAMA_CUBLAS'] = '1'
os.environ['CMAKE_CUDA_ARCHITECTURES'] = '89'

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(filename)s :%(lineno)d  %(asctime)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# 全局模型实例
llama_model = None

def get_gpu_info():
    """获取GPU信息"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for line in lines:
                parts = line.split(', ')
                if len(parts) >= 5:
                    name, total, used, temp, util = parts
                    return f"🔥 RTX 4090状态: 利用率{util}%, 显存{used}/{total}GB, 温度{temp}°C"
    except Exception as e:
        logger.warning(f"获取GPU信息失败: {e}")
    return "🔥 GPU状态: 未知"

def find_models():
    """查找可用模型"""
    model_dir = "/runpod-volume/text_models"
    models = []
    
    if os.path.exists(model_dir):
        for file in os.listdir(model_dir):
            if file.endswith('.gguf'):
                path = os.path.join(model_dir, file)
                size = os.path.getsize(path) / (1024**3)  # GB
                models.append((path, size))
                logger.info(f"📁 发现模型: {path} ({size:.1f}GB)")
    
    # 按大小排序，RTX 4090优先选择较大的模型
    models.sort(key=lambda x: x[1], reverse=True)
    return models

def get_rtx4090_config(model_path: str) -> dict:
    """RTX 4090专用配置"""
    model_size = os.path.getsize(model_path) / (1024**3)
    
    if model_size > 18:  # L3.2-8X3B (18.2GB)
        return {
            'n_gpu_layers': 45,  # 大模型用45层GPU
            'n_ctx': 16384,      # 16K上下文
            'n_batch': 512,
        }
    elif model_size > 13:  # L3.2-8X4B (13.9GB)
        return {
            'n_gpu_layers': 60,  # 中等模型用60层GPU
            'n_ctx': 32768,      # 32K上下文
            'n_batch': 1024,
        }
    else:  # 小模型
        return {
            'n_gpu_layers': -1,  # 所有层用GPU
            'n_ctx': 65536,      # 64K上下文
            'n_batch': 1024,
        }

def initialize_model():
    """初始化模型"""
    global llama_model
    
    logger.info("🔄 RTX 4090模型初始化...")
    logger.info(get_gpu_info())
    
    # 查找模型
    models = find_models()
    if not models:
        raise Exception("❌ 未找到任何.gguf模型文件")
    
    # RTX 4090优先选择最大的模型
    model_path, model_size = models[0]
    logger.info(f"🎯 RTX 4090选择模型: {model_path} ({model_size:.1f}GB)")
    
    try:
        # 导入llama_cpp
        import llama_cpp
        logger.info(f"llama-cpp-python版本: {llama_cpp.__version__}")
        logger.info(f"📂 RTX 4090强制GPU模式加载: {model_path}")
        
        # 获取RTX 4090专用配置
        config = get_rtx4090_config(model_path)
        logger.info(f"🔧 RTX 4090配置: {config}")
        
        # RTX 4090优化配置
        llama_model = llama_cpp.Llama(
            model_path=model_path,
            n_gpu_layers=config['n_gpu_layers'],
            n_ctx=config['n_ctx'],
            n_batch=config['n_batch'],
            verbose=True,  # 显示详细日志
            use_mmap=True,
            use_mlock=False,
            n_threads=8,  # RTX 4090可以用更多线程
            f16_kv=True,  # 使用半精度节省显存
            offload_kqv=True,  # 卸载KV缓存到GPU
        )
        
        logger.info("✅ RTX 4090模型初始化成功")
        logger.info(get_gpu_info())
        
        # 验证GPU使用
        test_response = llama_model("Test", max_tokens=1, echo=False)
        logger.info("🧪 RTX 4090 GPU测试完成")
        
        return True
        
    except Exception as e:
        logger.error(f"❌ RTX 4090模型初始化失败: {e}")
        raise

def handler(event):
    """RTX 4090优化Handler"""
    global llama_model
    
    logger.info("🎯 RTX 4090 Handler调用")
    logger.info(f"📥 完整事件: {event}")
    
    try:
        # 提取输入
        job_input = event.get("input", {})
        prompt = job_input.get("prompt", "")
        max_tokens = job_input.get("max_tokens", 1000)
        temperature = job_input.get("temperature", 0.7)
        
        logger.info(f"📝 提取的提示词: '{prompt}'")
        
        # 初始化模型（如果需要）
        if llama_model is None:
            logger.info("🔄 RTX 4090模型未初始化，开始初始化...")
            initialize_model()
        
        # 生成响应
        logger.info("🤖 RTX 4090开始生成响应...")
        start_time = time.time()
        
        response = llama_model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            echo=False,
            stop=["</s>", "<|eot_id|>", "<|end_of_text|>"]
        )
        
        generation_time = time.time() - start_time
        generated_text = response['choices'][0]['text'].strip()
        
        logger.info(f"✅ RTX 4090生成完成，耗时: {generation_time:.2f}秒")
        logger.info(f"📤 生成文本长度: {len(generated_text)}字符")
        logger.info(get_gpu_info())
        
        return {
            "text": generated_text,
            "generation_time": generation_time,
            "model_info": "RTX 4090 GPU加速模式",
            "gpu_status": get_gpu_info()
        }
        
    except Exception as e:
        logger.error(f"❌ RTX 4090 Handler执行失败: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    logger.info("🚀 启动RTX 4090专用RunPod handler...")
    logger.info(get_gpu_info())
    
    # 预初始化模型
    try:
        initialize_model()
    except Exception as e:
        logger.error(f"RTX 4090预初始化失败: {e}")
    
    # 启动RunPod
    runpod.serverless.start({"handler": handler})
EOF

RUN chmod +x /app/handler_rtx4090.py

# 创建模型目录
RUN mkdir -p /runpod-volume/text_models

# RTX 4090验证脚本
COPY <<EOF /app/verify_rtx4090.py
#!/usr/bin/env python3
import sys
import subprocess
import os

def verify_rtx4090():
    print("🔍 验证RTX 4090设置...")
    
    # 检查架构
    arch = subprocess.run(['uname', '-m'], capture_output=True, text=True).stdout.strip()
    print(f"📊 系统架构: {arch}")
    if arch != 'x86_64':
        print("❌ 错误：不是x86_64架构！")
        return False
    
    # 检查GPU型号
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_name = result.stdout.strip()
            print(f"🎯 检测到GPU: {gpu_name}")
            if 'RTX 4090' not in gpu_name:
                print(f"⚠️ 警告：不是RTX 4090，而是 {gpu_name}")
        else:
            print("❌ 无法检测GPU型号")
            return False
    except:
        print("❌ nvidia-smi命令失败")
        return False
    
    # 检查计算能力
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=compute_cap', '--format=csv,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            compute_cap = result.stdout.strip()
            print(f"🔧 计算能力: {compute_cap}")
            if compute_cap != '8.9':
                print(f"⚠️ 警告：计算能力不是8.9，而是 {compute_cap}")
    except:
        print("⚠️ 无法检测计算能力")
    
    # 检查llama-cpp-python
    try:
        import llama_cpp
        print(f"✅ llama-cpp-python版本: {llama_cpp.__version__}")
    except ImportError:
        print("❌ llama-cpp-python未安装")
        return False
    
    # 检查环境变量
    print("📊 RTX 4090环境变量:")
    expected_vars = {
        'GGML_CUDA': '1',
        'CUDA_VISIBLE_DEVICES': '0',
        'CMAKE_CUDA_ARCHITECTURES': '89'
    }
    
    for var, expected in expected_vars.items():
        value = os.environ.get(var, 'NOT SET')
        status = "✅" if value == expected else "❌"
        print(f"  {status} {var}: {value} (期望: {expected})")
    
    print("✅ RTX 4090验证完成")
    return True

if __name__ == "__main__":
    success = verify_rtx4090()
    sys.exit(0 if success else 1)
EOF

RUN chmod +x /app/verify_rtx4090.py

# 运行RTX 4090验证
RUN python3 /app/verify_rtx4090.py

# RTX 4090启动脚本
COPY <<EOF /app/start_rtx4090.sh
#!/bin/bash
set -e

echo "🚀 启动RTX 4090专用AI文本生成服务"
echo "📊 系统信息:"
echo "  - 容器架构: $(uname -m)"
echo "  - Python版本: $(python3 --version)"

echo "📊 RTX 4090 GPU信息:"
nvidia-smi --query-gpu=name,memory.total,compute_cap,driver_version --format=csv

echo "📊 RTX 4090环境变量:"
echo "  - GGML_CUDA: $GGML_CUDA"
echo "  - CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "  - CMAKE_CUDA_ARCHITECTURES: $CMAKE_CUDA_ARCHITECTURES"

echo "🧪 RTX 4090最终验证:"
python3 /app/verify_rtx4090.py

echo "🚀 启动RTX 4090专用Handler"
exec python3 /app/handler_rtx4090.py
EOF

RUN chmod +x /app/start_rtx4090.sh

# 端口
EXPOSE 8000

# 启动RTX 4090服务
CMD ["/app/start_rtx4090.sh"] 