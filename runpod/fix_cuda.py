#!/usr/bin/env python3
"""
ä¿®å¤CUDAæ”¯æŒè„šæœ¬ - é‡æ–°ç¼–è¯‘llama-cpp-python
"""

import os
import subprocess
import sys
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶è®°å½•è¾“å‡º"""
    logger.info(f"ğŸ”„ {description}")
    logger.info(f"æ‰§è¡Œå‘½ä»¤: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=300)
        
        if result.stdout:
            logger.info(f"è¾“å‡º: {result.stdout}")
        if result.stderr:
            logger.warning(f"é”™è¯¯: {result.stderr}")
            
        if result.returncode == 0:
            logger.info(f"âœ… {description} æˆåŠŸ")
        else:
            logger.error(f"âŒ {description} å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        logger.error(f"âŒ {description} è¶…æ—¶")
        return False
    except Exception as e:
        logger.error(f"âŒ {description} å¼‚å¸¸: {e}")
        return False

def main():
    logger.info("ğŸš€ å¼€å§‹ä¿®å¤CUDAæ”¯æŒ...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    os.environ['GGML_CUDA'] = '1'
    os.environ['CMAKE_ARGS'] = '-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=89'
    os.environ['FORCE_CMAKE'] = '1'
    
    logger.info("ğŸ”§ è®¾ç½®ç¯å¢ƒå˜é‡:")
    for key in ['CUDA_VISIBLE_DEVICES', 'GGML_CUDA', 'CMAKE_ARGS', 'FORCE_CMAKE']:
        logger.info(f"  {key}={os.environ.get(key)}")
    
    # 1. æ£€æŸ¥CUDA
    if not run_command("nvidia-smi", "æ£€æŸ¥CUDA"):
        logger.error("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")
        return False
    
    # 2. å¸è½½ç°æœ‰çš„llama-cpp-python
    run_command("pip uninstall llama-cpp-python -y", "å¸è½½ç°æœ‰llama-cpp-python")
    
    # 3. æ¸…ç†ç¼“å­˜
    run_command("pip cache purge", "æ¸…ç†pipç¼“å­˜")
    
    # 4. é‡æ–°å®‰è£…llama-cpp-python with CUDA
    install_cmd = (
        "CMAKE_ARGS='-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=89' "
        "FORCE_CMAKE=1 "
        "pip install --no-cache-dir --force-reinstall --verbose "
        "llama-cpp-python==0.3.9"
    )
    
    if not run_command(install_cmd, "é‡æ–°å®‰è£…llama-cpp-python with CUDA"):
        logger.error("âŒ å®‰è£…å¤±è´¥")
        return False
    
    # 5. éªŒè¯å®‰è£…
    logger.info("ğŸ” éªŒè¯CUDAæ”¯æŒ...")
    
    try:
        import llama_cpp
        logger.info(f"âœ… llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        
        # å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹æ¥æµ‹è¯•CUDA
        logger.info("ğŸ§ª æµ‹è¯•CUDAæ”¯æŒ...")
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        model_paths = [
            "/runpod-volume/text_models/L3.2-8X4B.gguf",
            "/runpod-volume/text_models/L3.2-8X3B.gguf"
        ]
        
        model_path = None
        for path in model_paths:
            if os.path.exists(path):
                model_path = path
                break
        
        if model_path:
            logger.info(f"ğŸ“‚ ä½¿ç”¨æ¨¡å‹: {model_path}")
            
            # æµ‹è¯•GPUæ¨¡å¼
            from llama_cpp import Llama
            
            model = Llama(
                model_path=model_path,
                n_ctx=512,
                n_batch=128,
                n_gpu_layers=1,  # åªæµ‹è¯•1å±‚
                verbose=True
            )
            
            logger.info("âœ… CUDAæ”¯æŒæµ‹è¯•æˆåŠŸï¼")
            del model
            
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè·³è¿‡CUDAæµ‹è¯•")
            
    except Exception as e:
        logger.error(f"âŒ CUDAæ”¯æŒéªŒè¯å¤±è´¥: {e}")
        return False
    
    logger.info("ğŸ‰ CUDAä¿®å¤å®Œæˆï¼")
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 