import os
import gc
import json
import runpod
import psutil
from typing import Optional, Dict, Any
import time

# RTX 4090ä¸“ç”¨é…ç½®
RTX4090_VRAM_GB = 24  # RTX 4090æœ‰24GBæ˜¾å­˜
RTX4090_COMPUTE_CAPABILITY = 8.9

# è®¾ç½®RTX 4090ä¸“ç”¨ç¯å¢ƒå˜é‡
os.environ.update({
    'GGML_CUDA': '1',
    'CUDA_VISIBLE_DEVICES': '0',
    'CUDA_LAUNCH_BLOCKING': '1',
    'CMAKE_CUDA_ARCHITECTURES': '89',  # RTX 4090çš„è®¡ç®—èƒ½åŠ›
})

try:
    import llama_cpp
    LLAMA_CPP_AVAILABLE = True
    print(f"âœ… llama_cpp version: {llama_cpp.__version__}")
except ImportError as e:
    LLAMA_CPP_AVAILABLE = False
    print(f"âŒ Failed to import llama_cpp: {e}")

class RTX4090ModelManager:
    """RTX 4090ä¸“ç”¨æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.current_model = None
        self.current_model_path = None
        self.rtx4090_config = {
            'n_gpu_layers': 60,  # RTX 4090å¯ä»¥å¤„ç†æ›´å¤šå±‚
            'n_ctx': 32768,      # RTX 4090çš„å¤§æ˜¾å­˜æ”¯æŒæ›´å¤§ä¸Šä¸‹æ–‡
            'verbose': True,
            'use_mmap': True,
            'use_mlock': False,
            'low_vram': False,   # RTX 4090ä¸éœ€è¦ä½æ˜¾å­˜æ¨¡å¼
        }
        
    def get_rtx4090_config(self, model_path: str) -> Dict[str, Any]:
        """æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´RTX 4090é…ç½®"""
        config = self.rtx4090_config.copy()
        config['model_path'] = model_path
        
        # æ ¹æ®æ–‡ä»¶åè°ƒæ•´é…ç½®
        if "8X3B" in model_path:
            # è¾ƒå¤§æ¨¡å‹ï¼Œä¿å®ˆä¸€ç‚¹
            config['n_gpu_layers'] = 50
            config['n_ctx'] = 16384
        elif "8X4B" in model_path:
            # æ›´å¤§æ¨¡å‹ï¼Œéœ€è¦æ›´ä¿å®ˆ
            config['n_gpu_layers'] = 45
            config['n_ctx'] = 8192
        else:
            # é»˜è®¤é…ç½®é€‚åˆRTX 4090
            config['n_gpu_layers'] = 55
            config['n_ctx'] = 16384
            
        print(f"ğŸ¯ RTX 4090é…ç½®: {config}")
        return config
        
    def load_model(self, model_path: str) -> bool:
        """åŠ è½½æ¨¡å‹åˆ°RTX 4090"""
        if not LLAMA_CPP_AVAILABLE:
            print("âŒ llama_cppæœªå®‰è£…")
            return False
            
        # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒæ¨¡å‹ï¼Œç›´æ¥è¿”å›
        if self.current_model and self.current_model_path == model_path:
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {model_path}")
            return True
            
        # æ¸…ç†æ—§æ¨¡å‹
        if self.current_model:
            print("ğŸ—‘ï¸ æ¸…ç†æ—§æ¨¡å‹...")
            del self.current_model
            gc.collect()
            
        try:
            print(f"ğŸš€ åŠ è½½æ¨¡å‹åˆ°RTX 4090: {model_path}")
            config = self.get_rtx4090_config(model_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return False
                
            start_time = time.time()
            self.current_model = llama_cpp.Llama(**config)
            load_time = time.time() - start_time
            
            self.current_model_path = model_path
            print(f"âœ… RTX 4090æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f}ç§’")
            print(f"ğŸ“Š GPUå±‚æ•°: {config['n_gpu_layers']}")
            print(f"ğŸ“Š ä¸Šä¸‹æ–‡é•¿åº¦: {config['n_ctx']}")
            
            return True
            
        except Exception as e:
            print(f"âŒ RTX 4090æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.current_model = None
            self.current_model_path = None
            return False
            
    def generate_response(self, prompt: str, max_tokens: int = 2048) -> str:
        """ä½¿ç”¨RTX 4090ç”Ÿæˆå›å¤"""
        if not self.current_model:
            return "âŒ æ¨¡å‹æœªåŠ è½½"
            
        try:
            # æ¸…ç†æç¤ºè¯ä¸­çš„é‡å¤BOSæ ‡è®°
            prompt = prompt.replace("<|begin_of_text|><|begin_of_text|>", "<|begin_of_text|>")
            prompt = prompt.replace("<|begin_of_text|>", "")
            
            # æ ¼å¼åŒ–æç¤ºè¯
            formatted_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            
            print(f"ğŸ¯ RTX 4090ç”Ÿæˆå¼€å§‹ï¼Œæœ€å¤§tokens: {max_tokens}")
            start_time = time.time()
            
            response = self.current_model(
                formatted_prompt,
                max_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                stop=["<|eot_id|>", "<|end_of_text|>"],
                echo=False
            )
            
            generation_time = time.time() - start_time
            
            if response and 'choices' in response and len(response['choices']) > 0:
                text = response['choices'][0]['text'].strip()
                tokens_generated = response['usage']['completion_tokens']
                
                print(f"âœ… RTX 4090ç”Ÿæˆå®Œæˆ")
                print(f"ğŸ“Š ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
                print(f"ğŸ“Š ç”Ÿæˆtokens: {tokens_generated}")
                print(f"ğŸ“Š é€Ÿåº¦: {tokens_generated/generation_time:.2f} tokens/ç§’")
                
                return text
            else:
                return "âŒ RTX 4090ç”Ÿæˆå¤±è´¥ï¼šæ— æœ‰æ•ˆå›å¤"
                
        except Exception as e:
            print(f"âŒ RTX 4090ç”Ÿæˆé”™è¯¯: {e}")
            return f"âŒ ç”Ÿæˆé”™è¯¯: {str(e)}"

# å…¨å±€RTX 4090æ¨¡å‹ç®¡ç†å™¨
rtx4090_manager = RTX4090ModelManager()

def handler(job):
    """RunPod RTX 4090ä¸“ç”¨å¤„ç†å‡½æ•°"""
    print("ğŸš€ RTX 4090 Handlerå¯åŠ¨")
    
    try:
        # è·å–è¾“å…¥å‚æ•°
        job_input = job.get("input", {})
        prompt = job_input.get("prompt", "")
        model_path = job_input.get("model_path", "/runpod-volume/text_models/L3.2-8X3B.gguf")
        max_tokens = job_input.get("max_tokens", 2048)
        
        if not prompt.strip():
            return {
                "status": "FAILED",
                "error": "ç©ºæç¤ºè¯"
            }
        
        print(f"ğŸ“ æç¤ºè¯: {prompt[:100]}...")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"ğŸ¯ æœ€å¤§tokens: {max_tokens}")
        
        # åŠ è½½æ¨¡å‹åˆ°RTX 4090
        if not rtx4090_manager.load_model(model_path):
            return {
                "status": "FAILED",
                "error": "RTX 4090æ¨¡å‹åŠ è½½å¤±è´¥"
            }
        
        # ä½¿ç”¨RTX 4090ç”Ÿæˆå›å¤
        response = rtx4090_manager.generate_response(prompt, max_tokens)
        
        if response.startswith("âŒ"):
            return {
                "status": "FAILED",
                "error": response
            }
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        memory_info = psutil.virtual_memory()
        
        return {
            "status": "COMPLETED",
            "output": {
                "response": response,
                "model_used": model_path,
                "gpu_type": "RTX 4090",
                "system_info": {
                    "memory_usage_percent": memory_info.percent,
                    "available_memory_gb": round(memory_info.available / (1024**3), 2)
                }
            }
        }
        
    except Exception as e:
        print(f"âŒ RTX 4090 Handleré”™è¯¯: {e}")
        return {
            "status": "FAILED",
            "error": f"å¤„ç†é”™è¯¯: {str(e)}"
        }

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨RTX 4090 RunPodæœåŠ¡å™¨")
    print(f"ğŸ“Š RTX 4090æ˜¾å­˜: {RTX4090_VRAM_GB}GB")
    print(f"ğŸ“Š è®¡ç®—èƒ½åŠ›: {RTX4090_COMPUTE_CAPABILITY}")
    
    runpod.serverless.start({"handler": handler}) 