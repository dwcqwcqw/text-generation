#!/usr/bin/env python3
"""
RTX 4090ä¸“ç”¨CUDAä¿®å¤è„šæœ¬
è§£å†³Ada Lovelaceæ¶æ„(è®¡ç®—èƒ½åŠ›8.9)çš„ç¼–è¯‘é—®é¢˜
"""

import os
import sys
import subprocess
import platform

def run_command(cmd, description=""):
    """æ‰§è¡Œå‘½ä»¤å¹¶æ£€æŸ¥ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {cmd}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=300)
        print("STDOUT:", result.stdout)
        if result.stderr:
            print("STDERR:", result.stderr)
        
        if result.returncode != 0:
            print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            return False
        else:
            print("âœ… å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            return True
    except subprocess.TimeoutExpired:
        print("âŒ å‘½ä»¤æ‰§è¡Œè¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ å‘½ä»¤æ‰§è¡Œå‡ºé”™: {e}")
        return False

def check_gpu():
    """æ£€æŸ¥GPUç±»å‹å’ŒCUDAç¯å¢ƒ"""
    print("\nğŸ” æ£€æŸ¥GPUå’ŒCUDAç¯å¢ƒ...")
    
    # æ£€æŸ¥nvidia-smi
    if not run_command("nvidia-smi", "æ£€æŸ¥NVIDIAé©±åŠ¨"):
        print("âŒ NVIDIAé©±åŠ¨æœªæ­£ç¡®å®‰è£…")
        return False
    
    # æ£€æŸ¥GPUå‹å·
    gpu_check = subprocess.run("nvidia-smi --query-gpu=name --format=csv,noheader,nounits", 
                              shell=True, capture_output=True, text=True)
    if gpu_check.returncode == 0:
        gpu_name = gpu_check.stdout.strip()
        print(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {gpu_name}")
        
        if "4090" in gpu_name:
            print("âœ… ç¡®è®¤ä¸ºRTX 4090ï¼Œè®¡ç®—èƒ½åŠ›8.9")
            return True
        else:
            print("âš ï¸  éRTX 4090ï¼Œä½†ä»å¯ä½¿ç”¨æ­¤è„šæœ¬")
            return True
    
    return False

def setup_rtx4090_environment():
    """è®¾ç½®RTX 4090ä¸“ç”¨ç¯å¢ƒå˜é‡"""
    print("\nğŸ”§ è®¾ç½®RTX 4090ä¸“ç”¨ç¯å¢ƒå˜é‡...")
    
    # RTX 4090 Ada Lovelaceæ¶æ„çš„è®¡ç®—èƒ½åŠ›æ˜¯8.9
    env_vars = {
        'GGML_CUDA': '1',
        'CUDA_VISIBLE_DEVICES': '0',
        'CUDA_LAUNCH_BLOCKING': '1',
        'CMAKE_ARGS': '-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=89 -DCMAKE_CUDA_COMPILER_FORCED=ON',
        'FORCE_CMAKE': '1',
        'CMAKE_CUDA_ARCHITECTURES': '89',  # RTX 4090ä¸“ç”¨
        'NVCC_APPEND_FLAGS': '--allow-unsupported-compiler',
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"âœ… è®¾ç½® {key}={value}")
    
    return True

def uninstall_llama_cpp():
    """å¸è½½ç°æœ‰çš„llama-cpp-python"""
    print("\nğŸ—‘ï¸  å¸è½½ç°æœ‰çš„llama-cpp-python...")
    
    commands = [
        "pip uninstall llama-cpp-python -y",
        "pip cache purge"
    ]
    
    for cmd in commands:
        run_command(cmd, f"æ‰§è¡Œ: {cmd}")

def install_rtx4090_optimized():
    """å®‰è£…RTX 4090ä¼˜åŒ–ç‰ˆæœ¬çš„llama-cpp-python"""
    print("\nğŸ“¦ å®‰è£…RTX 4090ä¼˜åŒ–ç‰ˆæœ¬çš„llama-cpp-python...")
    
    # ä½¿ç”¨æ˜¾å¼çš„è®¡ç®—èƒ½åŠ›å‚æ•°
    install_cmd = '''CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=89 -DCMAKE_CUDA_COMPILER_FORCED=ON" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --verbose'''
    
    if not run_command(install_cmd, "å®‰è£…RTX 4090ä¼˜åŒ–ç‰ˆæœ¬"):
        print("âŒ ç¬¬ä¸€æ¬¡å®‰è£…å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
        backup_cmd = '''CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=89;86;80;75" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir'''
        
        if not run_command(backup_cmd, "å¤‡ç”¨å®‰è£…æ–¹æ¡ˆ"):
            print("âŒ æ‰€æœ‰å®‰è£…æ–¹æ¡ˆéƒ½å¤±è´¥äº†")
            return False
    
    return True

def verify_rtx4090_installation():
    """éªŒè¯RTX 4090çš„å®‰è£…"""
    print("\nğŸ§ª éªŒè¯RTX 4090å®‰è£…...")
    
    test_script = '''
import sys
print("Pythonè·¯å¾„:", sys.executable)

try:
    import llama_cpp
    print("âœ… llama_cppå¯¼å…¥æˆåŠŸ")
    print(f"ç‰ˆæœ¬: {llama_cpp.__version__}")
    
    # æ£€æŸ¥CUDAæ”¯æŒ
    model_path = "/runpod-volume/text_models/L3.2-8X3B.gguf"
    
    try:
        llama = llama_cpp.Llama(
            model_path=model_path,
            n_gpu_layers=1,  # åªæµ‹è¯•1å±‚
            verbose=True,
            n_ctx=512  # å°ä¸Šä¸‹æ–‡æµ‹è¯•
        )
        print("âœ… RTX 4090 GPUåŠ è½½æµ‹è¯•æˆåŠŸï¼")
        print(f"GPUå±‚æ•°: {llama.n_gpu_layers}")
        
        # é‡Šæ”¾èµ„æº
        del llama
        
    except Exception as e:
        print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        
        # å°è¯•CPUæ¨¡å¼éªŒè¯å®‰è£…
        try:
            llama_cpu = llama_cpp.Llama(
                model_path=model_path,
                n_gpu_layers=0,
                verbose=True,
                n_ctx=512
            )
            print("âœ… CPUæ¨¡å¼å·¥ä½œæ­£å¸¸ï¼Œä½†GPUåŠ è½½å¤±è´¥")
            del llama_cpu
        except Exception as e2:
            print(f"âŒ è¿CPUæ¨¡å¼éƒ½å¤±è´¥: {e2}")

except ImportError as e:
    print(f"âŒ llama_cppå¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    print(f"âŒ å…¶ä»–é”™è¯¯: {e}")
'''
    
    return run_command(f'python3 -c "{test_script}"', "éªŒè¯RTX 4090å®‰è£…")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RTX 4090ä¸“ç”¨CUDAä¿®å¤è„šæœ¬å¯åŠ¨")
    print("ç›®æ ‡ï¼šè§£å†³Ada Lovelaceæ¶æ„(è®¡ç®—èƒ½åŠ›8.9)çš„ç¼–è¯‘é—®é¢˜")
    
    # æ£€æŸ¥GPU
    if not check_gpu():
        print("âŒ GPUæ£€æŸ¥å¤±è´¥")
        return False
    
    # è®¾ç½®ç¯å¢ƒ
    if not setup_rtx4090_environment():
        print("âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥")
        return False
    
    # å¸è½½æ—§ç‰ˆæœ¬
    uninstall_llama_cpp()
    
    # å®‰è£…RTX 4090ä¼˜åŒ–ç‰ˆæœ¬
    if not install_rtx4090_optimized():
        print("âŒ RTX 4090ä¼˜åŒ–å®‰è£…å¤±è´¥")
        return False
    
    # éªŒè¯å®‰è£…
    if not verify_rtx4090_installation():
        print("âŒ RTX 4090å®‰è£…éªŒè¯å¤±è´¥")
        return False
    
    print("\nğŸ‰ RTX 4090 CUDAä¿®å¤å®Œæˆï¼")
    print("ç°åœ¨åº”è¯¥èƒ½æ­£ç¡®ä½¿ç”¨GPUåŠ é€Ÿäº†")
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 