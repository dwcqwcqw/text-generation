#!/usr/bin/env python3
"""
ç»¼åˆæ£€æµ‹å’Œä¿®å¤è„šæœ¬
è§£å†³å‰ç«¯æ¨¡å‹é€‰æ‹©å’ŒGPUå…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import subprocess
import platform
import json
import time

def run_command(cmd, description="", timeout=60):
    """æ‰§è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {cmd}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
        print("STDOUT:", result.stdout)
        if result.stderr:
            print("STDERR:", result.stderr)
        
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        print(f"âŒ å‘½ä»¤æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)")
        return False, "", "Timeout"
    except Exception as e:
        print(f"âŒ å‘½ä»¤æ‰§è¡Œå‡ºé”™: {e}")
        return False, "", str(e)

def detect_system_architecture():
    """æ£€æµ‹ç³»ç»Ÿæ¶æ„å’ŒGPUç¯å¢ƒ"""
    print("\nğŸ” ç³»ç»Ÿæ¶æ„æ£€æµ‹")
    
    # Pythonå¹³å°ä¿¡æ¯
    print(f"Pythonå¹³å°: {platform.platform()}")
    print(f"å¤„ç†å™¨æ¶æ„: {platform.processor()}")
    print(f"æœºå™¨ç±»å‹: {platform.machine()}")
    print(f"ç³»ç»Ÿ: {platform.system()}")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨å®¹å™¨ä¸­
    if os.path.exists('/.dockerenv'):
        print("âœ… è¿è¡Œåœ¨Dockerå®¹å™¨ä¸­")
    else:
        print("âš ï¸ ä¸åœ¨Dockerå®¹å™¨ä¸­")
    
    # æ£€æŸ¥CUDAç¯å¢ƒ
    success, stdout, stderr = run_command("nvidia-smi", "æ£€æŸ¥NVIDIAé©±åŠ¨")
    if success:
        print("âœ… NVIDIAé©±åŠ¨å¯ç”¨")
        
        # è·å–GPUä¿¡æ¯
        success, gpu_info, _ = run_command("nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv,noheader", "è·å–GPUè¯¦ç»†ä¿¡æ¯")
        if success:
            print(f"ğŸ¯ GPUä¿¡æ¯: {gpu_info.strip()}")
    else:
        print("âŒ NVIDIAé©±åŠ¨ä¸å¯ç”¨")
        return False
    
    # æ£€æŸ¥CUDAç‰ˆæœ¬
    success, cuda_version, _ = run_command("nvcc --version", "æ£€æŸ¥CUDAç‰ˆæœ¬")
    if success:
        print(f"âœ… CUDAç‰ˆæœ¬: {cuda_version}")
    else:
        print("âŒ CUDAç¼–è¯‘å™¨ä¸å¯ç”¨")
    
    return True

def detect_llama_cpp_issues():
    """æ£€æµ‹llama-cpp-pythonçš„é—®é¢˜"""
    print("\nğŸ” llama-cpp-pythoné—®é¢˜æ£€æµ‹")
    
    try:
        import llama_cpp
        print(f"âœ… llama-cpp-pythonç‰ˆæœ¬: {llama_cpp.__version__}")
        
        # æ£€æŸ¥CUDAæ”¯æŒ
        test_script = '''
import llama_cpp
import os

# æµ‹è¯•CUDAæ”¯æŒ
try:
    # åˆ›å»ºä¸€ä¸ªæœ€å°çš„æµ‹è¯•æ¨¡å‹é…ç½®
    print("æµ‹è¯•CUDAæ”¯æŒ...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print(f"GGML_CUDA: {os.environ.get('GGML_CUDA', 'NOT SET')}")
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'NOT SET')}")
    
    # å°è¯•åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆä¸åŠ è½½å®é™…æ¨¡å‹ï¼‰
    print("llama-cpp-pythonå¯¼å…¥æˆåŠŸ")
    
except Exception as e:
    print(f"CUDAæ”¯æŒæµ‹è¯•å¤±è´¥: {e}")
'''
        
        success, output, error = run_command(f'python3 -c "{test_script}"', "æµ‹è¯•CUDAæ”¯æŒ")
        if "CPU_AARCH64" in output or "CPU_AARCH64" in error:
            print("âŒ æ£€æµ‹åˆ°CPU_AARCH64é”™è¯¯ - è¿™æ˜¯æ¶æ„ä¸åŒ¹é…é—®é¢˜")
            return False
        
        return True
        
    except ImportError as e:
        print(f"âŒ llama-cpp-pythonå¯¼å…¥å¤±è´¥: {e}")
        return False

def fix_architecture_mismatch():
    """ä¿®å¤æ¶æ„ä¸åŒ¹é…é—®é¢˜"""
    print("\nğŸ”§ ä¿®å¤æ¶æ„ä¸åŒ¹é…é—®é¢˜")
    
    # è®¾ç½®æ­£ç¡®çš„ç¯å¢ƒå˜é‡
    env_vars = {
        'GGML_CUDA': '1',
        'CUDA_VISIBLE_DEVICES': '0',
        'CMAKE_CUDA_ARCHITECTURES': 'auto',  # è®©ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹
        'FORCE_CMAKE': '1',
    }
    
    # æ£€æµ‹GPUç±»å‹å¹¶è®¾ç½®å¯¹åº”çš„æ¶æ„
    success, gpu_info, _ = run_command("nvidia-smi --query-gpu=name --format=csv,noheader", "è·å–GPUå‹å·")
    if success:
        gpu_name = gpu_info.strip().lower()
        if "4090" in gpu_name:
            env_vars['CMAKE_CUDA_ARCHITECTURES'] = '89'
            print("ğŸ¯ æ£€æµ‹åˆ°RTX 4090ï¼Œè®¾ç½®è®¡ç®—èƒ½åŠ›8.9")
        elif "l40" in gpu_name:
            env_vars['CMAKE_CUDA_ARCHITECTURES'] = '89'
            print("ğŸ¯ æ£€æµ‹åˆ°L40ï¼Œè®¾ç½®è®¡ç®—èƒ½åŠ›8.9")
        elif "3090" in gpu_name or "3080" in gpu_name:
            env_vars['CMAKE_CUDA_ARCHITECTURES'] = '86'
            print("ğŸ¯ æ£€æµ‹åˆ°RTX 30ç³»åˆ—ï¼Œè®¾ç½®è®¡ç®—èƒ½åŠ›8.6")
        else:
            env_vars['CMAKE_CUDA_ARCHITECTURES'] = '75;80;86;89'
            print("ğŸ¯ æœªçŸ¥GPUï¼Œä½¿ç”¨é€šç”¨æ¶æ„è®¾ç½®")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"âœ… è®¾ç½® {key}={value}")
    
    return True

def reinstall_llama_cpp():
    """é‡æ–°å®‰è£…llama-cpp-python"""
    print("\nğŸ“¦ é‡æ–°å®‰è£…llama-cpp-python")
    
    # å¸è½½ç°æœ‰ç‰ˆæœ¬
    run_command("pip uninstall llama-cpp-python -y", "å¸è½½ç°æœ‰ç‰ˆæœ¬")
    run_command("pip cache purge", "æ¸…ç†pipç¼“å­˜")
    
    # è·å–CMAKEå‚æ•°
    cuda_arch = os.environ.get('CMAKE_CUDA_ARCHITECTURES', 'auto')
    cmake_args = f"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES={cuda_arch}"
    
    # å®‰è£…æ–°ç‰ˆæœ¬
    install_cmd = f'CMAKE_ARGS="{cmake_args}" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --verbose'
    
    success, output, error = run_command(install_cmd, "å®‰è£…CUDAç‰ˆæœ¬", timeout=300)
    
    if not success:
        print("âŒ CUDAç‰ˆæœ¬å®‰è£…å¤±è´¥ï¼Œå°è¯•CPUç‰ˆæœ¬")
        success, output, error = run_command("pip install llama-cpp-python --no-cache-dir", "å®‰è£…CPUç‰ˆæœ¬")
    
    return success

def test_gpu_loading():
    """æµ‹è¯•GPUåŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•GPUåŠ è½½")
    
    test_script = '''
import llama_cpp
import os

try:
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_paths = [
        "/runpod-volume/text_models/L3.2-8X3B.gguf",
        "/runpod-volume/text_models/L3.2-8X4B.gguf"
    ]
    
    available_model = None
    for path in model_paths:
        if os.path.exists(path):
            available_model = path
            print(f"âœ… æ‰¾åˆ°æ¨¡å‹: {path}")
            break
    
    if not available_model:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        exit(1)
    
    # æµ‹è¯•GPUåŠ è½½
    print("æµ‹è¯•GPUåŠ è½½...")
    model = llama_cpp.Llama(
        model_path=available_model,
        n_gpu_layers=1,  # åªæµ‹è¯•1å±‚
        verbose=True,
        n_ctx=512
    )
    
    print("âœ… GPUåŠ è½½æµ‹è¯•æˆåŠŸï¼")
    print(f"æ¨¡å‹è·¯å¾„: {available_model}")
    
    # æ¸…ç†
    del model
    
except Exception as e:
    print(f"âŒ GPUåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
    exit(1)
'''
    
    success, output, error = run_command(f'python3 -c "{test_script}"', "GPUåŠ è½½æµ‹è¯•")
    
    if success and "assigned to device CUDA" in output:
        print("âœ… GPUåŠ è½½æˆåŠŸï¼")
        return True
    elif "assigned to device CPU" in output:
        print("âš ï¸ æ¨¡å‹åŠ è½½åˆ°CPUï¼ŒGPUæœªè¢«ä½¿ç”¨")
        return False
    else:
        print("âŒ GPUåŠ è½½æµ‹è¯•å¤±è´¥")
        return False

def fix_frontend_models():
    """ä¿®å¤å‰ç«¯æ¨¡å‹é€‰æ‹©é—®é¢˜"""
    print("\nğŸ”§ ä¿®å¤å‰ç«¯æ¨¡å‹é€‰æ‹©é—®é¢˜")
    
    # æ£€æŸ¥å‰ç«¯æ–‡ä»¶
    frontend_file = "/app/frontend/src/app/page.tsx"
    if not os.path.exists(frontend_file):
        frontend_file = "frontend/src/app/page.tsx"
    
    if os.path.exists(frontend_file):
        print(f"âœ… æ‰¾åˆ°å‰ç«¯æ–‡ä»¶: {frontend_file}")
        
        # å¼ºåˆ¶æ¸…ç†å‰ç«¯ç¼“å­˜
        frontend_dir = os.path.dirname(frontend_file)
        cache_dirs = [
            os.path.join(frontend_dir, ".next"),
            os.path.join(frontend_dir, "node_modules/.cache"),
            os.path.join(frontend_dir, ".cache")
        ]
        
        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):
                run_command(f"rm -rf {cache_dir}", f"æ¸…ç†ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # é‡æ–°æ„å»ºå‰ç«¯
        if os.path.exists(os.path.join(frontend_dir, "package.json")):
            run_command(f"cd {frontend_dir} && npm run build", "é‡æ–°æ„å»ºå‰ç«¯")
        
        return True
    else:
        print("âŒ æœªæ‰¾åˆ°å‰ç«¯æ–‡ä»¶")
        return False

def generate_diagnostic_report():
    """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
    print("\nğŸ“Š ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š")
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "system": {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "machine": platform.machine(),
            "python_version": sys.version
        },
        "environment": {},
        "gpu_info": {},
        "llama_cpp_status": {}
    }
    
    # æ”¶é›†ç¯å¢ƒå˜é‡
    for key in os.environ:
        if any(keyword in key.upper() for keyword in ['CUDA', 'GGML', 'CMAKE', 'RUNPOD']):
            report["environment"][key] = os.environ[key]
    
    # æ”¶é›†GPUä¿¡æ¯
    success, gpu_info, _ = run_command("nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv", "æ”¶é›†GPUä¿¡æ¯")
    if success:
        report["gpu_info"]["nvidia_smi"] = gpu_info
    
    # æ”¶é›†llama-cpp-pythonçŠ¶æ€
    try:
        import llama_cpp
        report["llama_cpp_status"]["version"] = llama_cpp.__version__
        report["llama_cpp_status"]["imported"] = True
    except ImportError as e:
        report["llama_cpp_status"]["imported"] = False
        report["llama_cpp_status"]["error"] = str(e)
    
    # ä¿å­˜æŠ¥å‘Š
    with open("diagnostic_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print("âœ… è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ° diagnostic_report.json")
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç»¼åˆæ£€æµ‹å’Œä¿®å¤è„šæœ¬å¯åŠ¨")
    print("ç›®æ ‡ï¼šè§£å†³å‰ç«¯æ¨¡å‹é€‰æ‹©å’ŒGPUå…¼å®¹æ€§é—®é¢˜")
    
    # 1. ç³»ç»Ÿæ¶æ„æ£€æµ‹
    if not detect_system_architecture():
        print("âŒ ç³»ç»Ÿæ¶æ„æ£€æµ‹å¤±è´¥")
        return False
    
    # 2. llama-cpp-pythoné—®é¢˜æ£€æµ‹
    llama_cpp_ok = detect_llama_cpp_issues()
    
    # 3. å¦‚æœæœ‰é—®é¢˜ï¼Œè¿›è¡Œä¿®å¤
    if not llama_cpp_ok:
        print("ğŸ”§ æ£€æµ‹åˆ°llama-cpp-pythoné—®é¢˜ï¼Œå¼€å§‹ä¿®å¤...")
        
        # ä¿®å¤æ¶æ„ä¸åŒ¹é…
        fix_architecture_mismatch()
        
        # é‡æ–°å®‰è£…
        if not reinstall_llama_cpp():
            print("âŒ llama-cpp-pythoné‡æ–°å®‰è£…å¤±è´¥")
            return False
    
    # 4. æµ‹è¯•GPUåŠ è½½
    gpu_ok = test_gpu_loading()
    
    # 5. ä¿®å¤å‰ç«¯é—®é¢˜
    fix_frontend_models()
    
    # 6. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
    report = generate_diagnostic_report()
    
    # 7. æ€»ç»“
    print("\nğŸ¯ ä¿®å¤æ€»ç»“:")
    print(f"âœ… ç³»ç»Ÿæ¶æ„: æ­£å¸¸")
    print(f"{'âœ…' if llama_cpp_ok else 'âŒ'} llama-cpp-python: {'æ­£å¸¸' if llama_cpp_ok else 'å·²ä¿®å¤'}")
    print(f"{'âœ…' if gpu_ok else 'âŒ'} GPUåŠ è½½: {'æ­£å¸¸' if gpu_ok else 'éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥'}")
    print(f"âœ… å‰ç«¯æ¨¡å‹: å·²ä¿®å¤")
    
    if gpu_ok:
        print("\nğŸ‰ æ‰€æœ‰é—®é¢˜å·²è§£å†³ï¼")
        return True
    else:
        print("\nâš ï¸ GPUé—®é¢˜ä»éœ€è¿›ä¸€æ­¥æ£€æŸ¥ï¼Œè¯·æŸ¥çœ‹è¯Šæ–­æŠ¥å‘Š")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 