#!/usr/bin/env python3
"""
RunPod GPUæœ€ç»ˆä¿®å¤è„šæœ¬
è§£å†³CPU_AARCH64é”™è¯¯å’ŒGPUå±‚åˆ†é…é—®é¢˜
"""

import os
import sys
import subprocess
import logging
import platform

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    logger.info("ğŸ” ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥:")
    logger.info(f"  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    logger.info(f"  æ¶æ„: {platform.machine()}")
    logger.info(f"  Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥CUDA
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            logger.info(f"  CUDAç‰ˆæœ¬: {result.stdout.split('release')[1].split(',')[0].strip()}")
        else:
            logger.warning("  CUDAæœªå®‰è£…æˆ–ä¸å¯ç”¨")
    except FileNotFoundError:
        logger.warning("  nvccå‘½ä»¤æœªæ‰¾åˆ°")
    
    # æ£€æŸ¥GPU
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            for line in result.stdout.strip().split('\n'):
                logger.info(f"  GPU: {line}")
        else:
            logger.warning("  GPUä¿¡æ¯è·å–å¤±è´¥")
    except FileNotFoundError:
        logger.warning("  nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°")

def force_x86_64_environment():
    """å¼ºåˆ¶è®¾ç½®x86_64ç¯å¢ƒ"""
    logger.info("ğŸ”§ å¼ºåˆ¶è®¾ç½®x86_64ç¯å¢ƒå˜é‡...")
    
    env_vars = {
        # CUDAç›¸å…³
        'CUDA_VISIBLE_DEVICES': '0',
        'GGML_CUDA': '1',
        'LLAMA_CUBLAS': '1',
        'CMAKE_CUDA_ARCHITECTURES': '75;80;86;89',
        'FORCE_CMAKE': '1',
        'CMAKE_ARGS': '-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=75;80;86;89',
        
        # æ¶æ„å¼ºåˆ¶
        'ARCHFLAGS': '-arch x86_64',
        'CFLAGS': '-march=x86-64',
        'CXXFLAGS': '-march=x86-64',
        'LDFLAGS': '-arch x86_64',
        
        # Pythonç›¸å…³
        'PYTHONPATH': '/usr/local/lib/python3.10/site-packages',
        'PIP_PREFER_BINARY': '1',
        
        # ç¼–è¯‘ç›¸å…³
        'CC': 'gcc',
        'CXX': 'g++',
        'NVCC_APPEND_FLAGS': '--allow-unsupported-compiler',
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        logger.info(f"  è®¾ç½® {key}={value}")

def install_dependencies():
    """å®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–"""
    logger.info("ğŸ”„ å®‰è£…RunPodä¾èµ–...")
    
    packages = [
        ('GPUtil', 'GPUtil'),
        ('runpod', 'runpod'),
    ]
    
    success_count = 0
    for package_name, import_name in packages:
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
            __import__(import_name)
            logger.info(f"âœ… {package_name} å·²å®‰è£…")
            success_count += 1
        except ImportError:
            # å®‰è£…åŒ…
            logger.info(f"ğŸ“¦ å®‰è£… {package_name}...")
            try:
                result = subprocess.run([
                    sys.executable, '-m', 'pip', 'install', package_name
                ], capture_output=True, text=True, check=True)
                logger.info(f"âœ… {package_name} å®‰è£…æˆåŠŸ")
                success_count += 1
            except subprocess.CalledProcessError as e:
                logger.error(f"âŒ {package_name} å®‰è£…å¤±è´¥: {e.stderr}")
    
    return success_count == len(packages)

def reinstall_llama_cpp_cuda():
    """é‡æ–°å®‰è£…CUDAç‰ˆæœ¬çš„llama-cpp-python"""
    logger.info("ğŸ”„ é‡æ–°å®‰è£…CUDAç‰ˆæœ¬çš„llama-cpp-python...")
    
    try:
        # å¸è½½ç°æœ‰ç‰ˆæœ¬
        logger.info("  å¸è½½ç°æœ‰ç‰ˆæœ¬...")
        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', 'llama-cpp-python', '-y'], 
                      check=False)
        
        # æ¸…ç†ç¼“å­˜
        logger.info("  æ¸…ç†pipç¼“å­˜...")
        subprocess.run([sys.executable, '-m', 'pip', 'cache', 'purge'], check=False)
        
        # å®‰è£…CUDAç‰ˆæœ¬
        logger.info("  å®‰è£…CUDAç‰ˆæœ¬...")
        cmd = [
            sys.executable, '-m', 'pip', 'install', 
            '--force-reinstall', '--no-cache-dir', '--no-deps',
            'llama-cpp-python', 
            '--extra-index-url', 'https://abetlen.github.io/llama-cpp-python/whl/cu121'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            logger.info("âœ… CUDAç‰ˆæœ¬å®‰è£…æˆåŠŸ")
            return True
        else:
            logger.error(f"âŒ å®‰è£…å¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ é‡æ–°å®‰è£…å¤±è´¥: {e}")
        return False

def test_gpu_loading():
    """æµ‹è¯•GPUåŠ è½½"""
    logger.info("ğŸ§ª æµ‹è¯•GPUåŠ è½½...")
    
    try:
        from llama_cpp import Llama
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•
        test_code = '''
import os
os.environ['GGML_CUDA'] = '1'
from llama_cpp import Llama

# æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
import glob
models = glob.glob("/runpod-volume/text_models/*.gguf")
if not models:
    print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
    exit(1)

model_path = models[0]
print(f"ğŸ“‚ æµ‹è¯•æ¨¡å‹: {model_path}")

# å°è¯•åŠ è½½æ¨¡å‹
try:
    model = Llama(
        model_path=model_path,
        n_ctx=2048,
        n_batch=512,
        n_gpu_layers=-1,  # å¼ºåˆ¶æ‰€æœ‰å±‚åˆ°GPU
        verbose=True,
        n_threads=1,
        use_mmap=True,
        f16_kv=True,
        main_gpu=0,
    )
    print("âœ… GPUåŠ è½½æµ‹è¯•æˆåŠŸ")
    
    # ç®€å•æ¨ç†æµ‹è¯•
    response = model("Hello", max_tokens=10, temperature=0.1)
    print(f"ğŸ“¤ æ¨ç†æµ‹è¯•: {response}")
    
except Exception as e:
    print(f"âŒ GPUåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
    exit(1)
'''
        
        # å†™å…¥æµ‹è¯•æ–‡ä»¶
        with open('/tmp/gpu_test.py', 'w') as f:
            f.write(test_code)
        
        # è¿è¡Œæµ‹è¯•
        result = subprocess.run([sys.executable, '/tmp/gpu_test.py'], 
                              capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            logger.info("âœ… GPUåŠ è½½æµ‹è¯•é€šè¿‡")
            logger.info(f"è¾“å‡º: {result.stdout}")
            return True
        else:
            logger.error(f"âŒ GPUåŠ è½½æµ‹è¯•å¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        return False

def create_optimized_dockerfile():
    """åˆ›å»ºä¼˜åŒ–çš„Dockerfile"""
    logger.info("ğŸ“ åˆ›å»ºä¼˜åŒ–çš„Dockerfile...")
    
    dockerfile_content = '''# å¼ºåˆ¶x86_64æ¶æ„çš„CUDAåŸºç¡€é•œåƒ
FROM --platform=linux/amd64 nvidia/cuda:12.1-base-ubuntu22.04

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_VISIBLE_DEVICES=0
ENV GGML_CUDA=1
ENV LLAMA_CUBLAS=1
ENV CMAKE_CUDA_ARCHITECTURES="75;80;86;89"
ENV FORCE_CMAKE=1
ENV CMAKE_ARGS="-DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=75;80;86;89"
ENV ARCHFLAGS="-arch x86_64"
ENV CFLAGS="-march=x86-64"
ENV CXXFLAGS="-march=x86-64"
ENV NVCC_APPEND_FLAGS="--allow-unsupported-compiler"

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \\
    python3 python3-pip python3-dev \\
    build-essential cmake \\
    nvidia-cuda-toolkit \\
    && rm -rf /var/lib/apt/lists/*

# å‡çº§pip
RUN python3 -m pip install --upgrade pip

# å®‰è£…CUDAç‰ˆæœ¬çš„llama-cpp-python
RUN pip install --no-cache-dir \\
    llama-cpp-python \\
    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# å®‰è£…å…¶ä»–ä¾èµ–
RUN pip install --no-cache-dir \\
    runpod GPUtil

# å¤åˆ¶handler
COPY handler_llama_ai.py /app/handler.py
WORKDIR /app

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\
    CMD python3 -c "import llama_cpp; print('OK')" || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "-u", "handler.py"]
'''
    
    with open('Dockerfile.gpu_optimized', 'w') as f:
        f.write(dockerfile_content)
    
    logger.info("âœ… ä¼˜åŒ–Dockerfileå·²åˆ›å»º: Dockerfile.gpu_optimized")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹RunPod GPUæœ€ç»ˆä¿®å¤...")
    
    # 1. æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
    check_system_info()
    
    # 2. å¼ºåˆ¶è®¾ç½®ç¯å¢ƒ
    force_x86_64_environment()
    
    # 3. å®‰è£…åŸºç¡€ä¾èµ–
    if not install_dependencies():
        logger.error("âŒ åŸºç¡€ä¾èµ–å®‰è£…å¤±è´¥")
        return False
    
    # 4. é‡æ–°å®‰è£…CUDAç‰ˆæœ¬
    if not reinstall_llama_cpp_cuda():
        logger.error("âŒ CUDAç‰ˆæœ¬å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return False
    
    # 5. æµ‹è¯•GPUåŠ è½½
    if not test_gpu_loading():
        logger.error("âŒ GPUåŠ è½½æµ‹è¯•å¤±è´¥")
        return False
    
    # 6. åˆ›å»ºä¼˜åŒ–Dockerfile
    create_optimized_dockerfile()
    
    logger.info("ğŸ‰ RunPod GPUä¿®å¤å®Œæˆï¼")
    logger.info("ğŸ“‹ ä¸‹ä¸€æ­¥:")
    logger.info("  1. é‡å¯RunPodå®¹å™¨")
    logger.info("  2. æˆ–ä½¿ç”¨æ–°çš„Dockerfile.gpu_optimizedé‡æ–°æ„å»ºé•œåƒ")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 